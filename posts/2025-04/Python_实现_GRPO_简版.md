---
title: "Python 实现 GRPO 简版"
date: 2025-04-18T02:34:42Z
draft: ["false"]
tags: [
  "fetched",
  "数据STUDIO"
]
categories: ["Duty"]
---
Python 实现 GRPO 简版 by 数据STUDIO
------
<div><p data-mpa-powered-by="yiban.io"><span leaf=""><img data-ratio="0.1782178217821782" data-type="gif" data-w="606" data-src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI7ibSBAB7MzqzBARx8T3TBZruGNeET47qhWwjg4BqHeZ1x5mGWvyVXy4pJclLnUblwlbcScjxFGNP5A/640?wx_fmt=gif" src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI7ibSBAB7MzqzBARx8T3TBZruGNeET47qhWwjg4BqHeZ1x5mGWvyVXy4pJclLnUblwlbcScjxFGNP5A/640?wx_fmt=gif"></span><span leaf=""><br></span></p><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><section nodeleaf=""><img data-ratio="0.15255813953488373" data-type="gif" data-w="1075" data-src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI7ibWWVA1RARlHw5jnibnfd6JGic5gARmlWo6uUAWD1ibWdqOWynFaJMcVWXw42637bhaKOybTxQgib4DEQ/640?wx_fmt=gif" src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI7ibWWVA1RARlHw5jnibnfd6JGic5gARmlWo6uUAWD1ibWdqOWynFaJMcVWXw42637bhaKOybTxQgib4DEQ/640?wx_fmt=gif"></section><p data-tool="mdnice编辑器"><span leaf="">今天我们将深入探讨GRPO的实现。先简要介绍这一概念，讨论方法，然后开始具体实现。</span></p><p data-tool="mdnice编辑器"><span leaf=""><img data-imgfileid="100106564" data-type="png" data-ratio="0.5981481481481481" data-w="1080" data-src="https://mmbiz.qpic.cn/mmbiz_png/gX9JE5tiaI7icmiaFXCvSicjxy977LFVaPO6FFDjMrfRP0K4qyQ5rO0uhMqEjE3TwYFbtUnKjeQreiaExmVGUCt7gicA/640?wx_fmt=png&amp;from=appmsg" src="https://mmbiz.qpic.cn/mmbiz_png/gX9JE5tiaI7icmiaFXCvSicjxy977LFVaPO6FFDjMrfRP0K4qyQ5rO0uhMqEjE3TwYFbtUnKjeQreiaExmVGUCt7gicA/640?wx_fmt=png&amp;from=appmsg"></span></p><h2 data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><span></span><span><span leaf="">什么是GRPO？</span></span><span></span></h2><p data-tool="mdnice编辑器"><span leaf="">GRPO是一种训练技术，旨在通过捕捉特定偏好的奖励函数来优化语言模型。与其他强化学习方法（如PPO或RLHF）不同，GRPO不需要复杂的评判模型和大量计算资源，而是直接优化语言模型，并通过在生成的响应组内计算相对优势来实现目标。</span></p><h2 data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><span></span><span><span leaf="">GRPO的关键特点</span></span><span></span></h2><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">GRPO的独特之处</span></span><span></span></h3><p data-tool="mdnice编辑器"><span leaf="">GRPO是一种新兴的强化学习技术，相比传统方法具有以下优势：</span></p><ul><li><section><strong><span leaf="">直接优化</span></strong><span leaf="">：不同于需要独立奖励模型的方法，GRPO直接使用显式奖励函数优化语言模型。</span></section></li><li><section><strong><span leaf="">多奖励信号</span></strong><span leaf="">：可以定义多个奖励函数，针对生成内容的不同方面（如正确性、格式、风格）。</span></section></li><li><section><strong><span leaf="">探索效率</span></strong><span leaf="">：GRPO通过在训练过程中为每个提示生成多个补全内容，有效探索输出空间。</span></section></li></ul><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">奖励函数</span></span><span></span></h3><p data-tool="mdnice编辑器"><span leaf="">代码实现了多个协同工作的奖励函数，用于指导模型：</span></p><ul><li><section><strong><span leaf="">correctness_reward_func</span></strong><span leaf="">：当模型提取的答案与真实答案匹配时，奖励2.0分。这是事实正确性的主要学习信号。</span></section></li><li><section><strong><span leaf="">int_reward_func</span></strong><span leaf="">：当答案是数字时奖励0.5分，适用于数学问题，引导模型生成数值响应。</span></section></li><li><section><strong><span leaf="">soft_format_reward_func和strict_format_reward_func</span></strong><span leaf="">：奖励正确的XML格式（0.5分），教导模型使用正确的标签结构响应。</span></section></li><li><section><strong><span leaf="">xmlcount_reward_func</span></strong><span leaf="">：为每个正确使用的XML标签提供部分奖励（每个标签0.125分），形成平滑的学习梯度。</span></section></li></ul><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">实现组件</span></span><span></span></h3><ul><li><section><strong><span leaf="">奖励函数</span></strong><span leaf="">：根据特定标准评估模型输出：</span></section></li><ul><li><section><span leaf="">正确性：检查提取的答案是否与真实答案匹配。</span></section></li><li><section><span leaf="">格式遵循：确保响应符合请求的XML格式。</span></section></li><li><section><span leaf="">整数检测：奖励数值答案。</span></section></li></ul><li><section><strong><span leaf="">数据集准备</span></strong><span leaf="">：使用GSM8K（数学应用题）数据集，并进行特定格式化。</span></section></li><li><section><strong><span leaf="">训练配置</span></strong><span leaf="">：使用LoRA进行参数高效微调。</span></section></li></ul><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">训练过程</span></span><span></span></h3><ul><li><section><span leaf="">对于数据集中的每个提示，模型生成多个补全内容（由</span><strong><span leaf="">num_generations</span></strong><span leaf="">设置，代码中为4）。</span></section></li><li><section><span leaf="">每个补全内容由所有奖励函数评估。</span></section></li><li><section><span leaf="">奖励用于更新模型权重，鼓励模型生成更高奖励的输出。</span></section></li><li><section><span leaf="">此过程持续指定的周期数。</span></section></li></ul><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">参数高效微调</span></span><span></span></h3><p data-tool="mdnice编辑器"><span leaf="">我们使用LoRA（低秩适应）高效微调模型。LoRA向注意力层添加小型可训练的“适配器”矩阵，大幅减少训练参数数量（通常&gt;99%）。</span><strong><span leaf="">peft_config</span></strong><span leaf="">定义了目标层和适配器的秩。</span></p><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">实施考虑</span></span><span></span></h3><ul><li><section><span leaf="">使用较小的模型（Qwen2.5-1.5B-Instruct）以适应内存限制。</span></section></li><li><section><span leaf="">减小批次大小和生成数量以管理内存使用，并使用较小的数据集子集（20个示例）进行快速实验。</span></section></li><li><section><span leaf="">测试代码可在训练后立即评估结果。可通过增加</span><strong><span leaf="">max_samples</span></strong><span leaf="">行更全面的训练，或尝试不同的奖励函数。</span></section></li></ul><h2 data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><span></span><span><span leaf="">代码实现</span></span><span></span></h2><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">安装所需包</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">pip install -q transformers datasets trl peft accelerate</span><span leaf=""><br></span><span leaf="">import re</span><span leaf=""><br></span><span leaf="">import torch</span><span leaf=""><br></span><span leaf="">import numpy as np</span><span leaf=""><br></span><span leaf="">from datasets import load_dataset, Dataset</span><span leaf=""><br></span><span leaf="">from transformers import AutoTokenizer, AutoModelForCausalLM</span><span leaf=""><br></span><span leaf="">from peft import LoraConfig, get_peft_model</span><span leaf=""><br></span><span leaf="">from trl import GRPOConfig, GRPOTrainer</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">定义系统和响应提示</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">SYSTEM_PROMPT = </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">请按以下格式响应：  </span><span leaf=""><br></span><span leaf="">&lt;reasoning&gt;  </span><span leaf=""><br></span><span leaf="">...  </span><span leaf=""><br></span><span leaf="">&lt;/reasoning&gt;  </span><span leaf=""><br></span><span leaf="">&lt;answer&gt;  </span><span leaf=""><br></span><span leaf="">...  </span><span leaf=""><br></span><span leaf="">&lt;/answer&gt;  </span><span leaf=""><br></span><span leaf="">"</span></span><span><span leaf="">""</span></span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">XML_COT_FORMAT = </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">&lt;reasoning&gt;  </span><span leaf=""><br></span><span leaf="">{reasoning}  </span><span leaf=""><br></span><span leaf="">&lt;/reasoning&gt;  </span><span leaf=""><br></span><span leaf="">&lt;answer&gt;  </span><span leaf=""><br></span><span leaf="">{answer}  </span><span leaf=""><br></span><span leaf="">&lt;/answer&gt;  </span><span leaf=""><br></span><span leaf="">"</span></span><span><span leaf="">""</span></span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">提取答案的辅助函数</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">def extract_xml_answer(text: str) -&gt; str:  </span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"从XML格式的响应中提取答案部分。"</span></span><span><span leaf="">""</span></span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">if</span></span><span><span leaf="">"&lt;answer&gt;"</span></span><span leaf=""> not </span><span><span leaf="">in</span></span><span leaf=""> text or </span><span><span leaf="">"&lt;/answer&gt;"</span></span><span leaf=""> not </span><span><span leaf="">in</span></span><span leaf=""> text:  </span><span leaf=""><br></span><span leaf="">        </span><span><span leaf="">return</span></span><span><span leaf="">""</span></span><span leaf=""><br></span><span leaf="">    answer = text.split(</span><span><span leaf="">"&lt;/answer&gt;"</span></span><span leaf="">)[-1]  </span><span leaf=""><br></span><span leaf="">    answer = answer.split(</span><span><span leaf="">"&lt;answer&gt;"</span></span><span leaf="">)[0]  </span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> answer.strip()  </span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">def extract_hash_answer(text: str) -&gt; str:  </span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"从GSM8K格式中提取答案（###标记之后）。"</span></span><span><span leaf="">""</span></span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">if</span></span><span><span leaf="">"####"</span></span><span leaf=""> not </span><span><span leaf="">in</span></span><span leaf=""> text:  </span><span leaf=""><br></span><span leaf="">        </span><span><span leaf="">return</span></span><span><span leaf="">""</span></span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> text.split(</span><span><span leaf="">"###"</span></span><span leaf="">)[1].strip().replace(</span><span><span leaf="">"."</span></span><span leaf="">, </span><span><span leaf="">""</span></span><span leaf="">).replace(</span><span><span leaf="">"$"</span></span><span leaf="">, </span><span><span leaf="">""</span></span><span leaf="">)</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">加载并准备GSM8K数据集</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">def get_gsm8k_questions(split=</span><span><span leaf="">"train"</span></span><span leaf="">, max_samples=100) -&gt; Dataset:  </span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    加载GSM8K数据集并格式化为GRPO训练所需形式。  </span><span leaf=""><br></span><span leaf="">    参数：  </span><span leaf=""><br></span><span leaf="">        split: 使用的数据集划分（train, test）  </span><span leaf=""><br></span><span leaf="">        max_samples: 使用的最大样本数（用于快速实验）  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    data = load_dataset(</span><span><span leaf="">'openai/gsm8k'</span></span><span leaf="">, </span><span><span leaf="">'main'</span></span><span leaf="">)[split]  </span><span leaf="">    </span><span><span leaf=""># 限制数据集大小以加快实验  </span></span><span leaf="">    </span><span><span leaf="">if</span></span><span leaf=""> max_samples and max_samples &lt; len(data):  </span><span leaf="">        data = data.select(range(max_samples))  </span><span leaf="">    </span><span><span leaf=""># 格式化数据为所需的提示结构  </span></span><span leaf="">    data = data.map(lambda x: {  </span><span leaf="">        </span><span><span leaf="">'prompt'</span></span><span leaf="">: [  </span><span leaf="">            {</span><span><span leaf="">'role'</span></span><span leaf="">: </span><span><span leaf="">'system'</span></span><span leaf="">, </span><span><span leaf="">'content'</span></span><span leaf="">: SYSTEM_PROMPT},  </span><span leaf="">            {</span><span><span leaf="">'role'</span></span><span leaf="">: </span><span><span leaf="">'user'</span></span><span leaf="">, </span><span><span leaf="">'content'</span></span><span leaf="">: x[</span><span><span leaf="">'question'</span></span><span leaf="">]}  </span><span leaf="">        ],  </span><span leaf="">        </span><span><span leaf="">'answer'</span></span><span leaf="">: extract_hash_answer(x[</span><span><span leaf="">'answer'</span></span><span leaf="">])  </span><span leaf="">    })  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> data</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">奖励函数</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">def correctness_reward_func(prompts, completions, answer, **kwargs):  </span><span leaf=""><br></span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    检查提取的答案是否与真实答案匹配的奖励函数。  </span><span leaf=""><br></span><span leaf="">    正确答案返回2.0，否则返回0.0。  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    responses = [completion[0][</span><span><span leaf="">'content'</span></span><span leaf="">] </span><span><span leaf="">for</span></span><span leaf=""> completion </span><span><span leaf="">in</span></span><span leaf=""> completions]  </span><span leaf="">    q = prompts[0][-1][</span><span><span leaf="">'content'</span></span><span leaf="">]  </span><span leaf="">    extracted_responses = [extract_xml_answer(r) </span><span><span leaf="">for</span></span><span leaf=""> r </span><span><span leaf="">in</span></span><span leaf=""> responses]  </span><span leaf="">    </span><span><span leaf=""># 打印调试信息  </span></span><span leaf="">    </span><span><span leaf="">if</span></span><span leaf=""> kwargs.get(</span><span><span leaf="">'debug'</span></span><span leaf="">, False) and len(responses) &gt; 0:  </span><span leaf="">        </span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">'-'</span></span><span leaf="">*20)  </span><span leaf="">        </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"问题：\n{q}"</span></span><span leaf="">)  </span><span leaf="">        </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"\n真实答案：\n{answer[0]}"</span></span><span leaf="">)  </span><span leaf="">        </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"\n模型响应：\n{responses[0]}"</span></span><span leaf="">)  </span><span leaf="">        </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"\n提取的答案：\n{extracted_responses[0]}"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> [2.0 </span><span><span leaf="">if</span></span><span leaf=""> r == a </span><span><span leaf="">else</span></span><span leaf=""> 0.0 </span><span><span leaf="">for</span></span><span leaf=""> r, a </span><span><span leaf="">in</span></span><span leaf=""> zip(extracted_responses, answer)]  </span><span leaf="">def int_reward_func(completions, **kwargs) -&gt; list[</span><span><span leaf="">float</span></span><span leaf="">]:  </span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    检查提取的答案是否为数字的奖励函数。  </span><span leaf=""><br></span><span leaf="">    整数答案返回0.5，否则返回0.0。  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    responses = [completion[0][</span><span><span leaf="">'content'</span></span><span leaf="">] </span><span><span leaf="">for</span></span><span leaf=""> completion </span><span><span leaf="">in</span></span><span leaf=""> completions]  </span><span leaf="">    extracted_responses = [extract_xml_answer(r) </span><span><span leaf="">for</span></span><span leaf=""> r </span><span><span leaf="">in</span></span><span leaf=""> responses]  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> [0.5 </span><span><span leaf="">if</span></span><span leaf=""> r.isdigit() </span><span><span leaf="">else</span></span><span leaf=""> 0.0 </span><span><span leaf="">for</span></span><span leaf=""> r </span><span><span leaf="">in</span></span><span leaf=""> extracted_responses]  </span><span leaf="">def strict_format_reward_func(completions, **kwargs) -&gt; list[</span><span><span leaf="">float</span></span><span leaf="">]:  </span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    检查补全内容是否完全符合格式的奖励函数。  </span><span leaf=""><br></span><span leaf="">    匹配格式返回0.5，否则返回0.0。  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    pattern = r</span><span><span leaf="">"^\n&lt;reasoning&gt;.*?&lt;/reasoning&gt;\n\n&lt;answer&gt;.*?&lt;/answer&gt;\n$"</span></span><span leaf="">    responses = [completion[0][</span><span><span leaf="">'content'</span></span><span leaf="">] </span><span><span leaf="">for</span></span><span leaf=""> completion </span><span><span leaf="">in</span></span><span leaf=""> completions]  </span><span leaf="">    matches = [bool(re.search(pattern, r, flags=re.DOTALL)) </span><span><span leaf="">for</span></span><span leaf=""> r </span><span><span leaf="">in</span></span><span leaf=""> responses]  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> [0.5 </span><span><span leaf="">if</span></span><span leaf=""> match </span><span><span leaf="">else</span></span><span leaf=""> 0.0 </span><span><span leaf="">for</span></span><span leaf=""> match </span><span><span leaf="">in</span></span><span leaf=""> matches]  </span><span leaf="">def soft_format_reward_func(completions, **kwargs) -&gt; list[</span><span><span leaf="">float</span></span><span leaf="">]:  </span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    宽松的格式检查奖励函数。  </span><span leaf=""><br></span><span leaf="">    匹配格式返回0.5，否则返回0.0。  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    pattern = r</span><span><span leaf="">"&lt;reasoning&gt;.*?&lt;/reasoning&gt;.*?&lt;answer&gt;.*?&lt;/answer&gt;"</span></span><span leaf="">    responses = [completion[0][</span><span><span leaf="">'content'</span></span><span leaf="">] </span><span><span leaf="">for</span></span><span leaf=""> completion </span><span><span leaf="">in</span></span><span leaf=""> completions]  </span><span leaf="">    matches = [bool(re.search(pattern, r, flags=re.DOTALL)) </span><span><span leaf="">for</span></span><span leaf=""> r </span><span><span leaf="">in</span></span><span leaf=""> responses]  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> [0.5 </span><span><span leaf="">if</span></span><span leaf=""> match </span><span><span leaf="">else</span></span><span leaf=""> 0.0 </span><span><span leaf="">for</span></span><span leaf=""> match </span><span><span leaf="">in</span></span><span leaf=""> matches]  </span><span leaf="">def count_xml(text) -&gt; </span><span><span leaf="">float</span></span><span leaf="">:  </span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    统计XML标签并为每个正确放置的标签提供部分奖励。  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    count = 0.0  </span><span leaf="">    </span><span><span leaf="">if</span></span><span leaf=""> text.count(</span><span><span leaf="">"&lt;reasoning&gt;"</span></span><span leaf="">) == 1:  </span><span leaf="">        count += 0.125  </span><span leaf="">    </span><span><span leaf="">if</span></span><span leaf=""> text.count(</span><span><span leaf="">"&lt;/reasoning&gt;"</span></span><span leaf="">) == 1:  </span><span leaf="">        count += 0.125  </span><span leaf="">    </span><span><span leaf="">if</span></span><span leaf=""> text.count(</span><span><span leaf="">"&lt;answer&gt;"</span></span><span leaf="">) == 1:  </span><span leaf="">        count += 0.125  </span><span leaf="">    </span><span><span leaf="">if</span></span><span leaf=""> text.count(</span><span><span leaf="">"&lt;/answer&gt;"</span></span><span leaf="">) == 1:  </span><span leaf="">        count += 0.125  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> count  </span><span leaf="">def xmlcount_reward_func(completions, **kwargs) -&gt; list[</span><span><span leaf="">float</span></span><span leaf="">]:  </span><span leaf="">    </span><span><span leaf="">""</span></span><span><span leaf="">"  </span><span leaf=""><br></span><span leaf="">    基于响应中XML标签计数的奖励函数。  </span><span leaf=""><br></span><span leaf="">    "</span></span><span><span leaf="">""</span></span><span leaf="">    contents = [completion[0][</span><span><span leaf="">"content"</span></span><span leaf="">] </span><span><span leaf="">for</span></span><span leaf=""> completion </span><span><span leaf="">in</span></span><span leaf=""> completions]  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> [count_xml(c) </span><span><span leaf="">for</span></span><span leaf=""> c </span><span><span leaf="">in</span></span><span leaf=""> contents]</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">模型设置</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">model_name = </span><span><span leaf="">"Qwen/Qwen2.5-1.5B-Instruct"</span></span><span leaf="">  </span><span leaf=""><br></span><span><span leaf=""># 设置输出目录和运行名称  </span></span><span leaf=""><br></span><span leaf="">output_dir = </span><span><span leaf="">"outputs/Qwen-1.5B-GRPO"</span></span><span leaf="">  </span><span leaf=""><br></span><span leaf="">run_name = </span><span><span leaf="">"Qwen-1.5B-GRPO-gsm8k"</span></span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">配置GRPO训练</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">training_args = GRPOConfig(  </span><span leaf=""><br></span><span leaf="">    output_dir=output_dir,  </span><span leaf="">    run_name=run_name,  </span><span leaf="">    learning_rate=5e-6,  </span><span leaf="">    adam_beta1=0.9,  </span><span leaf="">    adam_beta2=0.99,  </span><span leaf="">    weight_decay=0.1,  </span><span leaf="">    warmup_ratio=0.1,  </span><span leaf="">    lr_scheduler_type=</span><span><span leaf="">'cosine'</span></span><span leaf="">,  </span><span leaf="">    logging_steps=1,  </span><span leaf="">    bf16=False,  </span><span><span leaf=""># 设置为False，因为Colab不支持  </span></span><span leaf="">    fp16=True,   </span><span><span leaf=""># 使用fp16以提高兼容性  </span></span><span leaf="">    per_device_train_batch_size=4,  </span><span><span leaf=""># 增加以兼容GRPO  </span></span><span leaf="">    gradient_accumulation_steps=2,  </span><span leaf="">    num_generations=4,  </span><span><span leaf=""># 必须是per_device_train_batch_size的除数  </span></span><span leaf="">    max_prompt_length=256,  </span><span leaf="">    max_completion_length=512,  </span><span leaf="">    num_train_epochs=1,  </span><span leaf="">    save_steps=50,  </span><span leaf="">    max_grad_norm=0.1,  </span><span leaf="">    report_to=</span><span><span leaf="">"none"</span></span><span leaf="">,  </span><span leaf="">    log_on_each_node=False,  </span><span leaf="">)</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">配置LoRA进行参数高效微调</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span leaf="">peft_config = LoraConfig(  </span><span leaf=""><br></span><span leaf="">    r=8,  </span><span><span leaf=""># 从16减少以适应Colab内存  </span></span><span leaf=""><br></span><span leaf="">    lora_alpha=32,  </span><span leaf=""><br></span><span leaf="">    target_modules=[</span><span><span leaf="">"q_proj"</span></span><span leaf="">, </span><span><span leaf="">"k_proj"</span></span><span leaf="">, </span><span><span leaf="">"v_proj"</span></span><span leaf="">, </span><span><span leaf="">"o_proj"</span></span><span leaf="">],  </span><span><span leaf=""># 简化目标模块  </span></span><span leaf=""><br></span><span leaf="">    task_type=</span><span><span leaf="">"CAUSAL_LM"</span></span><span leaf="">,  </span><span leaf=""><br></span><span leaf="">    lora_dropout=0.05,  </span><span leaf=""><br></span><span leaf="">)</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">加载并准备模型</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"加载模型：{model_name}"</span></span><span leaf="">)  </span><span leaf="">model = AutoModelForCausalLM.from_pretrained(  </span><span leaf="">    model_name,  </span><span leaf="">    torch_dtype=torch.float16,  </span><span><span leaf=""># 使用float16而非bfloat16  </span></span><span leaf="">    device_map=</span><span><span leaf="">"auto"</span></span><span leaf="">,          </span><span><span leaf=""># 让模型自动选择最佳设备配置  </span></span><span leaf="">    low_cpu_mem_usage=True,    </span><span><span leaf=""># 提高内存效率  </span></span><span leaf="">    trust_remote_code=True     </span><span><span leaf=""># 新模型有时需要  </span></span><span leaf="">)  </span><span><span leaf=""># 加载分词器  </span></span><span leaf="">tokenizer = AutoTokenizer.from_pretrained(model_name)  </span><span><span leaf="">if</span></span><span leaf=""> tokenizer.pad_token is None:  </span><span leaf="">    tokenizer.pad_token = tokenizer.eos_token  </span><span><span leaf=""># 加载GSM8K数据集的子集  </span></span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"加载数据集..."</span></span><span leaf="">)  </span><span leaf="">dataset = get_gsm8k_questions(max_samples=20)  </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"数据集加载完成，共{len(dataset)}个示例"</span></span><span leaf="">)</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">初始化GRPO训练器</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"初始化GRPO训练器..."</span></span><span leaf="">)  </span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">trainer = GRPOTrainer(  </span><span leaf=""><br></span><span leaf="">    model=model,  </span><span leaf=""><br></span><span leaf="">    processing_class=tokenizer,  </span><span leaf=""><br></span><span leaf="">    reward_funcs=[  </span><span leaf=""><br></span><span leaf="">        xmlcount_reward_func,  </span><span leaf=""><br></span><span leaf="">        soft_format_reward_func,  </span><span leaf=""><br></span><span leaf="">        int_reward_func,  </span><span leaf=""><br></span><span leaf="">        correctness_reward_func  </span><span leaf=""><br></span><span leaf="">    ],  </span><span leaf=""><br></span><span leaf="">    args=training_args,  </span><span leaf=""><br></span><span leaf="">    train_dataset=dataset,  </span><span leaf=""><br></span><span leaf="">    peft_config=peft_config  </span><span><span leaf=""># 启用LoRA进行高效微调  </span></span><span leaf=""><br></span><span leaf="">)</span><span leaf=""><br></span></code></pre><h3 data-tool="mdnice编辑器"><span></span><span><span leaf="">运行GRPO</span></span><span></span></h3><pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><code><span><span leaf=""># 开始训练  </span></span><span leaf=""><br></span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"开始GRPO训练..."</span></span><span leaf="">)  </span><span leaf="">trainer.train()  </span><span><span leaf=""># 保存最终模型  </span></span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"训练完成。保存模型..."</span></span><span leaf="">)  </span><span leaf="">trainer.save_model()  </span><span><span leaf=""># 训练后测试模型  </span></span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"\n--- 测试训练后的模型 ---\n"</span></span><span leaf="">)  </span><span><span leaf=""># 生成预测的函数  </span></span><span leaf="">def generate_prediction(model, tokenizer, question, max_length=512):  </span><span leaf="">    prompt = [  </span><span leaf="">        {</span><span><span leaf="">'role'</span></span><span leaf="">: </span><span><span leaf="">'system'</span></span><span leaf="">, </span><span><span leaf="">'content'</span></span><span leaf="">: SYSTEM_PROMPT},  </span><span leaf="">        {</span><span><span leaf="">'role'</span></span><span leaf="">: </span><span><span leaf="">'user'</span></span><span leaf="">, </span><span><span leaf="">'content'</span></span><span leaf="">: question}  </span><span leaf="">    ]  </span><span leaf="">    </span><span><span leaf=""># 格式化提示  </span></span><span leaf="">    messages = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)  </span><span leaf="">    </span><span><span leaf=""># 分词输入  </span></span><span leaf="">    inputs = tokenizer(messages, return_tensors=</span><span><span leaf="">"pt"</span></span><span leaf="">).to(model.device)  </span><span leaf="">    </span><span><span leaf=""># 生成响应  </span></span><span leaf="">    with torch.no_grad():  </span><span leaf="">        outputs = model.generate(  </span><span leaf="">            **inputs,  </span><span leaf="">            max_new_tokens=max_length,  </span><span leaf="">            do_sample=False  </span><span leaf="">        )  </span><span leaf="">    </span><span><span leaf=""># 解码响应  </span></span><span leaf="">    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)  </span><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> response  </span><span><span leaf=""># 测试数据集中的几个示例  </span></span><span leaf="">test_examples = dataset.select(range(3))  </span><span><span leaf="">for</span></span><span leaf=""> i, example </span><span><span leaf="">in</span></span><span leaf=""> enumerate(test_examples):  </span><span leaf="">    question = example[</span><span><span leaf="">'prompt'</span></span><span leaf="">][-1][</span><span><span leaf="">'content'</span></span><span leaf="">]  </span><span leaf="">    ground_truth = example[</span><span><span leaf="">'answer'</span></span><span leaf="">]  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"\n示例 {i+1}:"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"问题：{question}"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"真实答案：{ground_truth}"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf=""># 生成预测  </span></span><span leaf="">    response = generate_prediction(model, tokenizer, question)  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"模型响应：{response}"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf=""># 提取答案  </span></span><span leaf="">    extracted_answer = extract_xml_answer(response)  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"提取的答案：{extracted_answer}"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(f</span><span><span leaf="">"是否正确：{extracted_answer == ground_truth}"</span></span><span leaf="">)  </span><span leaf="">    </span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"-"</span></span><span leaf=""> * 50)  </span><span><span leaf="">print</span></span><span leaf="">(</span><span><span leaf="">"搞定！"</span></span><span leaf="">)</span><span leaf=""><br></span></code></pre><h2 data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid=""></span><span></span><span><span leaf="">总结</span></span><span></span></h2><p data-tool="mdnice编辑器"><span leaf="">这一实现展示了GRPO的工作原理，以及如何利用它优化语言模型以适应特定格式和任务。数学问题解决任务与XML格式的结合，清晰地体现了该技术的能力。</span></p><p data-tool="mdnice编辑器"><span leaf="">真是一次有趣的实践！</span></p></section><blockquote><p><span leaf="">作者：arjun链接：https://www.k-a.in/grpo-1B.html</span></p><p><span leaf="">编辑：AI翻译、「深度学习自然语言处理」公众号润色</span></p></blockquote><section data-mpa-template="t" mpa-from-tpl="t"><section data-mid="" mpa-from-tpl="t"><section data-mid="" mpa-from-tpl="t" nodeleaf=""><img data-ratio="1.452" data-type="gif" data-w="250" data-src="https://mmbiz.qpic.cn/mmbiz_gif/BfjzYen78aJ9jibPqC4UskObNfOib0yfKAaAc91ATL5qjKhtYfWFzia902H2WxfqrGISHpcSnSLK4jyb2qJd54w8w/640?wx_fmt=gif" src="https://mmbiz.qpic.cn/mmbiz_gif/BfjzYen78aJ9jibPqC4UskObNfOib0yfKAaAc91ATL5qjKhtYfWFzia902H2WxfqrGISHpcSnSLK4jyb2qJd54w8w/640?wx_fmt=gif"></section><section data-mid="" mpa-from-tpl="t"><span leaf=""><br></span></section></section></section><h5><span><span leaf="">🏴‍☠️宝藏级🏴‍☠️ 原创公众号『</span><strong><span leaf="">数据STUDIO</span></strong><span leaf="">』内容超级硬核。公众号以Python为核心语言，垂直于数据科学领域，包括</span></span><span><span leaf="">可戳</span></span><span><span leaf="">👉</span><strong><span leaf=""> </span></strong></span><strong><span leaf=""><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0OTI1OTQ2MQ==&amp;action=getalbum&amp;album_id=1974978822768771072&amp;scene=173&amp;from_msgid=2247519294&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" textvalue="" target="_blank" linktype="text" data-linktype="2">Python</a></span></strong><span><strong><span><span leaf="">｜</span></span></strong></span><strong><span leaf=""><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0OTI1OTQ2MQ==&amp;action=getalbum&amp;album_id=2023684574089658370&amp;scene=173&amp;from_msgid=2247519619&amp;from_itemidx=2&amp;count=3&amp;nolastread=1#wechat_redirect" textvalue="" target="_blank" linktype="text" data-linktype="2">MySQL</a></span></strong><span><strong><span><span leaf="">｜</span></span></strong></span><strong><span leaf=""><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0OTI1OTQ2MQ==&amp;action=getalbum&amp;album_id=1974978820940054530&amp;scene=173&amp;from_msgid=2247518366&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" textvalue="" target="_blank" linktype="text" data-linktype="2">数据分析</a></span></strong><span><strong><span><span leaf="">｜</span></span></strong></span><strong><span leaf=""><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0OTI1OTQ2MQ==&amp;action=getalbum&amp;album_id=1974991176839544834&amp;scene=173&amp;from_msgid=2247519244&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" textvalue="" target="_blank" linktype="text" data-linktype="2">数据可视化</a></span></strong><span><strong><span><span leaf="">｜</span></span></strong></span><strong><span leaf=""><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0OTI1OTQ2MQ==&amp;action=getalbum&amp;album_id=1963494160565354497&amp;scene=173&amp;from_msgid=2247512171&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" textvalue="" target="_blank" linktype="text" data-linktype="2">机器学习与数据挖掘</a></span></strong><span><strong><span><span leaf="">｜</span></span></strong></span><strong><span leaf=""><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0OTI1OTQ2MQ==&amp;action=getalbum&amp;album_id=2318258648965644288&amp;scene=173&amp;from_msgid=2247518366&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" textvalue="" target="_blank" linktype="text" data-linktype="2">爬虫</a></span></strong><span><span leaf=""> </span><span><span leaf="">等，从入门到进阶！</span></span></span></h5><p><span><span leaf="">长按👇关注- 数据STUDIO -设为星标，干货速递</span></span><span leaf=""><img data-ratio="0.3351851851851852" data-type="gif" data-w="1080" data-src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI78ZgGwzt8M0xnekvrATwDWP7y4cdlwy4WOrJSUIqRfncsEYsPYM9wkQ8Gpr57zCpzia124Gb2d7icTQ/640?wx_fmt=gif" src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI78ZgGwzt8M0xnekvrATwDWP7y4cdlwy4WOrJSUIqRfncsEYsPYM9wkQ8Gpr57zCpzia124Gb2d7icTQ/640?wx_fmt=gif"><img data-ratio="0.08703703703703704" data-type="gif" data-w="1080" data-src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI7ickLkMzUmaxe4mdegUFLheymDmGI4OiaeeoY4K0ttDRGju4p6F7iaGfSb4H6EDCCCC9bo7KuLiblIdXQ/640?wx_fmt=gif" src="https://mmbiz.qpic.cn/mmbiz_gif/gX9JE5tiaI7ickLkMzUmaxe4mdegUFLheymDmGI4OiaeeoY4K0ttDRGju4p6F7iaGfSb4H6EDCCCC9bo7KuLiblIdXQ/640?wx_fmt=gif"></span></p><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/EL7ZvtJS7WEeWI5P53qHDA",target="_blank" rel="noopener noreferrer">原文链接</a>
