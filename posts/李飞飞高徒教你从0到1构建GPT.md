---
title: "李飞飞高徒教你从0到1构建GPT"
date: 2023-01-25T12:52:11Z
draft: ["false"]
tags: [
  "fetched",
  "Datawhale"
]
categories: ["Duty"]
---
李飞飞高徒教你从0到1构建GPT by Datawhale
------
<div><section data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" data-mpa-powered-by="yiban.io"><section data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><h3><section data-tools="135编辑器" data-id="88402"><section data-style="line-height: 1.8; text-align: justify; font-size: 15px; letter-spacing: 0px; color: rgb(117, 114, 114);white-space: normal;"><section><section><section data-tools="135编辑器" data-id="88402"><section data-style="line-height: 1.8; text-align: justify; font-size: 15px; letter-spacing: 0px; color: rgb(117, 114, 114);white-space: normal;"><section><section><section powered-by="xiumi.us"><section><section><section><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><section><section><section><section><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><section><span> Datawhale干货 </span></section><section><span><strong>教程：GPT，来源：量子位</strong></span><span></span></section></section></section></section></section></section></section></section></section></section></section></section></section></section><section><mp-common-profile data-pluginname="mpprofile" data-weui-theme="light" data-id="MzIyNjM2MzQyNg==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/0?wx_fmt=png" data-nickname="Datawhale" data-alias="Datawhale" data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from="2" data-is_biz_ban="0" data-index="0" data-origin_num="487" data-isban="0"></mp-common-profile><strong></strong></section></section></section></section></section></section></section></section></h3></section></section><section><strong><br>“从0到1手搓GPT”教程来了！</strong></section><p>视频1个多小时，从原理到代码都一一呈现，训练微调也涵盖在内，手把手带着你搞定。</p><p><img data-ratio="0.5633333333333334" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqy9UnL2v4LeCdIhnhicY9dHMUibPWx43xMnpW3XNJzf93lgetrMlX246Q/640?wx_fmt=jpeg" data-type="jpeg" data-w="600" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqy9UnL2v4LeCdIhnhicY9dHMUibPWx43xMnpW3XNJzf93lgetrMlX246Q/640?wx_fmt=jpeg"></p><p>该内容刚发出来，在Twitter已吸引400万关注量，HackerNews上Points也破了900。</p><p><strong>连马斯克也下场支持。</strong></p><p><img data-ratio="0.2554112554112554" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdq3G1UGKRkXkv1J2duhXtiaq2pdgElwcqtATgyoeYgq6CgPrDVlf97QmA/640?wx_fmt=jpeg" data-type="jpeg" data-w="924" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdq3G1UGKRkXkv1J2duhXtiaq2pdgElwcqtATgyoeYgq6CgPrDVlf97QmA/640?wx_fmt=jpeg"></p><p>有人评价，Andrej确实是一位出色的“事物解释者”，也热心于回答大家的问题。</p><p><img data-ratio="0.31411530815109345" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqj9cLBppWCNVoeibp2OQT2tBwlzBFmGEWVuC1yZJ8dw6UqeyoSVibkH7Q/640?wx_fmt=jpeg" data-type="jpeg" data-w="1006" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqj9cLBppWCNVoeibp2OQT2tBwlzBFmGEWVuC1yZJ8dw6UqeyoSVibkH7Q/640?wx_fmt=jpeg"></p><p>还有网友更夸张，称该教程简直是来“救命”。</p><p><img data-ratio="0.2484076433121019" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqmrfdIur42RZRia9YywcUlCK5Lxcx3mQ0lgxt8eqBaDmWmMh5hH5pkrg/640?wx_fmt=jpeg" data-type="jpeg" data-w="942" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqmrfdIur42RZRia9YywcUlCK5Lxcx3mQ0lgxt8eqBaDmWmMh5hH5pkrg/640?wx_fmt=jpeg"></p><p>那么，这位<strong>教程作者</strong>是谁？</p><p>正是前特斯拉AI总监，李飞飞高徒——<strong>Andrej Karpathy</strong>。</p><p><img data-ratio="0.5824074074074074" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqgRxNEeKm2Dic5ibrXtgP9p7xer43TH129cDsA9zL8TicXUFV6N1KMjO0A/640?wx_fmt=jpeg" data-type="jpeg" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqgRxNEeKm2Dic5ibrXtgP9p7xer43TH129cDsA9zL8TicXUFV6N1KMjO0A/640?wx_fmt=jpeg"></p><p><strong>教程具体说了什么？</strong></p><p>这就来展开讲讲。</p><h2>从零构建GPT，总共几步？</h2><p>视频教程先从理论讲起。</p><p>第一部分主要关于建立基准语言模型（二元）以及Transformer核心注意力机制，以及该机制内节点之间的信息传递，自注意力机制理论也有涉及。</p><p>该part内容长度超过1小时，不仅有概念解释，还教你如何使用矩阵乘法、添加softmax归一化，可谓“夯实基础”式讲解。</p><p><img data-ratio="0.7657407407407407" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdq1IFQXms53Up5kGoAia3VBBJ3pMfBda8fRtsdicmMu8rJphQIXW34urkQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdq1IFQXms53Up5kGoAia3VBBJ3pMfBda8fRtsdicmMu8rJphQIXW34urkQ/640?wx_fmt=jpeg"></p><p>接着讲述构建Transformer。</p><p>这当中涉及了多头注意力（包括如何插入自注意力构建块）、多层感知机（MLP）、残差连接、归一化方法LayerNorm以及如何在Transformer中添加Dropout Notes…….</p><p>然后，作者会带大家训练一个模型，当中会用到一个名为nanoGPT的库，可调用GPT-2参数，快速完成GPT模型的训练。</p><p>教程中，作者还将所得模型与Open AI的GPT-3比较。两者规模差距达1万-100万倍，但神经网络是相同的。另一个将拿来比较的是人尽皆知的ChatGPT，当然，我们目前所得只是预训练模型。</p><p><img data-ratio="0.4583333333333333" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqibxlmwoaNV6vc54SDXcYwrBnic9FrjD54jJX5rPnyUl2zvm7kOricE6GQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="1056" src="https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqibxlmwoaNV6vc54SDXcYwrBnic9FrjD54jJX5rPnyUl2zvm7kOricE6GQ/640?wx_fmt=jpeg"></p><p>在上述内容引导下，我们已得一个10M参数规模的模型，在一个GPU上训练15分钟，喂给1MB大小的莎士比亚文本数据集，它就能像莎士比亚一样输出。</p><p>比如下面两张图，你能分辨哪个是真人莎士比亚写的吗？</p><p><img data-ratio="0.43425925925925923" data-src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqVzbIMXCmibCxN2YlMzZBCKtHJr0DOdB9UgfOw3bpBKzDGruu6HR4emA/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqVzbIMXCmibCxN2YlMzZBCKtHJr0DOdB9UgfOw3bpBKzDGruu6HR4emA/640?wx_fmt=png"></p><p>评论区有人好奇选什么GPU资源。作者也分享了下——自己用的是Lambda的云上GPU，这是他目前接触按需计费GPU中，最简单的渠道。</p><p>光说不练不行，作者还给出一些课后练习，总共四道题，包括：</p><ul><li><p>N维张量掌握挑战；</p></li><li><p>在自己选择的数据集上训练GPT；</p></li><li><p>找一个非常大的数据集，基于它训练Transformer，然后初始化再基于莎士比亚数据集微调，看能否通过预训练获得更低的验证损失？</p></li><li><p>参考Transformer相关论文，看看之前研究中哪些操作能进一步提升性能；</p></li></ul><h2>神器nanoGPT发布</h2><p>前文提及，作者之所以能快速完成训练GPT，有赖于一个名nanoGPT的库。</p><p>这也是本教程作者前几天刚发布的利器，由2年前的minGPT升级而来，只是换了个更“标题党”的名字，自称纳米级<span>（nano）</span>。目前，其在GitHub所获star已超8k，网友连连点赞。</p><p><img data-ratio="0.4583333333333333" data-src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqrwtexD2AxP2luJ7lpnUtwWWmGVphsAs39yzqNvEbUuw8XERqfnK9aQ/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqrwtexD2AxP2luJ7lpnUtwWWmGVphsAs39yzqNvEbUuw8XERqfnK9aQ/640?wx_fmt=png"></p><p>据作者介绍，该库里面包含一个约300行的GPT模型定义（文件名：model.py），可以从OpenAI加载GPT-2权重。</p><p>还有一个训练模型PyTorch样板（文件名：train.py），同样也是300多行。</p><p>对想上手的AI玩家来说，无论是从头开始训练新模型，还是基于预训练进行微调（目前可用的最大模型为1.3B参数的GPT-2），各路需求均能满足。</p><p><img data-ratio="0.45555555555555555" data-src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqT7dNyXtZOlORXel3GL84FGMU4ZPwictOGAlNZb5x3WaicPzo1Ysthj5g/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqT7dNyXtZOlORXel3GL84FGMU4ZPwictOGAlNZb5x3WaicPzo1Ysthj5g/640?wx_fmt=png"><br><strong>△</strong><span> 一个训练实例展示</span></p><p>据作者目前自己的测试，他在1 个 A100 40GB GPU 上训练一晚，损失约为 3.74。如果是在4个GPU上训练损失约为3.60。</p><p>如果在8个A100 40GB节点上进行约50万次迭代，时长约为1天，atim的训练降至约3.1，init随机概率是10.82，已将结果带到了baseline范围。</p><p><img data-ratio="0.4744186046511628" data-src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqmUVQBXibdqQGQKDrqZMOgqkOxYNlI50h1ic9xrPEejkNK5GuaqtLFvuA/640?wx_fmt=png" data-type="png" data-w="860" src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDNrzqkzWFx3JThQmssUsdqmUVQBXibdqQGQKDrqZMOgqkOxYNlI50h1ic9xrPEejkNK5GuaqtLFvuA/640?wx_fmt=png"></p><p>对macbook或一些“力量”不足的小破本，靠nanoGPT也能开训。</p><p>不过，作者建议使用莎士比亚（shakespeare）数据集，该数据集前文已提及，大小约1MB，然后在一个很小的网络上运行。</p><p>据他自己亲身示范，创建了一个小得多的Transformer（4层，4个head，64嵌入大小），在作者自己的苹果AIR M1本上，每次迭代大约需要400毫秒。</p><p><span>（附视频和nanoGPT地址，有需要的朋友自取）</span><strong></strong></p><ul><li><p><span>课程视频：</span></p></li></ul><p>https://www.youtube.com/watch?v=kCc8FmEb1nY</p><ul><li><p><span>nanoGPT GitHub：</span></p></li></ul><p>https://github.com/karpathy/nanoGPT</p><p><span>参考链接：<br>[1]https://twitter.com/karpathy/status/1615398117683388417?s=46&amp;t=69hVy8CNcEBXBYmQHXhdxA<br>[2]https://news.ycombinator.com/item?id=34414716</span></p><p><span><img data-galleryid="" data-ratio="0.40555555555555556" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsF25MMFFibDibFZ6IrpZibxXibbtcZia26S7Pgq0OPSpoIw7XuzZiaWeCrXfBF4JD4XVoHY6BTiaPQQEdWFg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" data-type="png" data-w="900" data-backw="546" data-backh="221" src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsF25MMFFibDibFZ6IrpZibxXibbtcZia26S7Pgq0OPSpoIw7XuzZiaWeCrXfBF4JD4XVoHY6BTiaPQQEdWFg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"><span>整理不易，</span><span><strong><span>点</span></strong></span><span><strong><span>赞</span></strong></span><strong><span><span>三连</span></span></strong><span>↓</span></span></p><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/pszo5KZgusmQpjkYCT2ihg",target="_blank" rel="noopener noreferrer">原文链接</a>
