---
title: "DeepSeek R1 Zero中文复现教程来了！"
date: 2025-02-06T14:21:31Z
draft: ["false"]
tags: [
  "fetched",
  "Datawhale"
]
categories: ["Duty"]
---
DeepSeek R1 Zero中文复现教程来了！ by Datawhale
------
<div><section data-mpa-powered-by="yiban.io" data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><h1 data-tool="mdnice编辑器"><section><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><section><section powered-by="xiumi.us"><section><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><p><span><strong>作者</strong><strong>：</strong><strong>骆秀韬，Datawhale成员</strong></span></p></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname="mpprofile" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname="Datawhale" data-alias="Datawhale" data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from="2" data-weuitheme="light" data-origin_num="702" data-isban="0" data-biz_account_status="0" data-index="0" data-is_biz_ban="0"></mp-common-profile></section></section></section></section></section></h1></section><section><span>项目代码可见：unlock-deepseek/Datawhale-R1（https://github.com/datawhalechina/unlock-deepseek），欢迎关注和 star！</span></section><section><span><strong><span>其余所有开源内容见文末。</span></strong></span></section><hr><section>各位同学好，我是来自 Unlock-DeepSeek 开源项目团队的骆师傅。先说结论，<span>我们（Datawhale X 似然实验室）使用 3 张 A800(80G) 计算卡，花了 20 小时训练时间，做出了可能是国内首批 DeepSeek R1 Zero 的中文复现版本，我们把它叫做 Datawhale-R1，用于 R1 Zero 复现教学。</span>　</section><p><img data-imgfileid="100216634" data-ratio="0.3972222222222222" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUds9yrFwyfhXWBiauyA0pRkib9r9seYR79TQffkpraEEtb8HGLic3YcarQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUds9yrFwyfhXWBiauyA0pRkib9r9seYR79TQffkpraEEtb8HGLic3YcarQ/640?wx_fmt=png&amp;from=appmsg"></p><p>按照 5.5 元 ~ 7.0 元每小时的价格计算，3 张 A800 花费最低为 3 * 5.5 * 20 = 330 元，预计花费接近 420 元，而 <span>TinyZero</span><span>（https://github.com/Jiayi-Pan/TinyZero） </span>项目用了 4 张 A800 训练了 8 小时，预计花费为：224 元，这中间的差异可能是由于硬件性能瓶颈和框架差异带来的（我们用的是 Huggingface TRL，TinyZero 使用的是 veRL）。所以建议<span>大家如果真的要复现，请使用 TinyZero 项目</span>，我们出于教育目的使用 TRL 为大家报告这个结果。　</p><p>另外，不是所有人都能随时随地调用 3 张 A800 的，我们正在努力减小硬件资源要求，让复现工作尽可能平民化（比如在 4090 上跑）。在这里特别感谢：似然实验室，提供本次复现的计算资源，并与 Datawhale 团队合作贡献了本教程。　</p><section><mp-common-profile data-pluginname="mpprofile" data-id="MzI5NzQxODU2Nw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/lT7vzwjGsCheD71q8AZsAuppFnRUmy5JSBHMhoI0fUto4TEV9pwvIremTGpPs1V0YvHeV0WlebqQVuBUDibJwqA/0?wx_fmt=png" data-nickname="似然实验室" data-alias="LikelihoodLab" data-signature="人工智能实验室，我们相信AI 改变世界，让世界更加美好。" data-from="0" data-is_biz_ban="0" data-service_type="1"></mp-common-profile></section><p>回到正题，首先回答一个关键问题：为什么这个方案更贵，而我们却选择了它？答案就是：它更符合教育目的，截止本文发布，大部分同学没有足够的资源来亲手体验复现流程，但是我们希望大家能更清楚的看到，复现 R1 Zero 的过程中都发生了什么，真正对复现原理有个大致把握，就算做“云玩家”也要学到知识，看完骆师傅做一遍就好像自己也做了一遍。　</p><section><p>本方案在 <span>mini-r1</span><span>（https://www.philschmid.de/mini-deepseek-r1）</span>的基础上改进而来。　</p></section><h1>环境搭建</h1><h2>配置基础工具</h2><p>首先我们要搭建环境，作为手把手教程以及骆师傅的看家本领，我们会在这部分说得细致些。结合国内的实际情况，我们需要的环境信息如下：　</p><section><p>暂时无法支持非 Linux 系统（Windows、MacOS）　</p></section><ul><li><section><span>CUDA &gt; 12.0 （我们使用的是 CUDA 12.4）</span></section></li><li><section><span>Python 建议版本为 3.12（我们使用 Miniforge 管理虚拟环境）</span></section></li><li><section><span>Pytorch 版本为 2.5.1 (GPU版本，请使用 </span><span>torch.cuda.is_available()</span><span> 检查能否正常识别 GPU 设备）</span></section></li></ul><section><p>建议使用 Miniforge / Conda 来安装 Pytorch，我们在南方科技大学的开源镜像源测试，下载速度会比官网 pip 安装快不少，请在下面的网址找到适合你硬件的 2.5.1 版本：https://pytorch.org/get-started/previous-versions/，推荐使用 mamba 安装（安装 Miniforge 后直接将 conda 替换为 mamba）　</p></section><h2>编译安装 flash-attn</h2><p>接着重头戏就来了，我们需要编译安装 Flash Attention 包，这步非常消耗 CPU 资源，非常不建议CPU核心少的玩家执行。如果你没有办法在“有生之年”编译完 Flash Attention，可以在 https://github.com/Dao-AILab/flash-attention/releases/ 找到与你环境对应的编译好的包。（没对应上的话，改环境反而更快，相信我，编译很慢）　</p><p>这个步骤倒是很简单，执行下面的命令：　</p><section><p><br></p><pre><p>pip install packaging<br>pip install ninja <span># 用于加速编译</span><br><br><span># 编译安装 Flash Attention 包</span><br>pip install flash-attn --no-build-isolation<br><br><span># 注意！如果你的设备CPU核心多，但是运行内存小于 96 GB，请适当设置 MAX_JOBS 的数量，并替换为下面的命令，参考：https://github.com/Dao-AILab/flash-attention#installation-and-features</span><br>MAX_JOBS=4 pip install flash-attn --no-build-isolation</p></pre></section><p>按下回车后，可以泡杯咖啡，打开 <span>htop</span> 看 CPU 疯狂运作，再重新品读一遍<span>《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》</span><span>（https://arxiv.org/abs/2501.12948）　</span></p><p>等待 flash-attn 安装完毕后，我们就可以安装其他涉及到的库了，我们提供了一份 <span>requirements.txt</span> 在<span> Unlock-DeepSeek</span><span>（https://github.com/datawhalechina/unlock-deepseek）</span>项目，核心列表如下：　</p><section><p><br></p><pre><p>setuptools&lt;<span>71.0.0</span><br>transformers==<span>4.48.1</span><br>datasets==<span>3.1.0</span><br>accelerate==<span>1.3.0</span><br>hf-transfer==<span>0.1.9</span><br>deepspeed==<span>0.15.4</span><br>trl==<span>0.14.0</span><br>vllm==<span>0.7.0</span><br>modelscope==<span>1.22.3</span><br>swanlab==<span>0.4.6</span><br>huggingface-hub==<span>0.28.1</span></p></pre></section><section><p>大家也可以在这个地址找到我们所有涉及的 Python 包列表：https://swanlab.cn/@anine09/datawhale-r1/runs/4tp31j1zxbm1fshjsi53b/environment/requirements　</p></section><h2>下载模型和数据集</h2><p>接下来我们需要下载数据集和模型，在本次实验中，我们使用的数据集为：<span>Jiayi-Pan/Countdown-Tasks-3to4</span><span>（https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4）</span>，模型为：<span>Qwen/Qwen2.5-3B-Instruct</span><span>（https://huggingface.co/Qwen/Qwen2.5-3B-Instruct）</span>，我们目前不建议用小于 3B 的模型（其他社区多次报告，小于 3B 的模型无法学会推理，经过我们的测试，确实！）　</p><p>数据集下载方式：　</p><section><p><br></p><pre><p><span>export</span> HF_ENDPOINT=https://hf-mirror.com <span># 更换为国内镜像源，这个只用执行一次，每次重新打开终端就要重新执行，或者写入 .bashrc</span><br><br><span># 下载数据集，替换整个 &lt;xxx&gt; 为你自己的内容</span><br>huggingface-cli download --repo-type dataset --resume-download Jiayi-Pan/Countdown-Tasks-3to4 --local-dir &lt;你想要存放的路径，比如：dataset&gt;</p></pre></section><p>模型下载方式，哪个速度快用哪个：　</p><ul><li><section><span>方案一，Huggingface 镜像源</span></section></li></ul><section><p><br></p><pre><p><span># 下载模型，替换整个 &lt;xxx&gt; 为你自己的内容</span><br>huggingface-cli download --resume-download Qwen/Qwen2.5-3B-Instruct --local-dir &lt;你想要存放的路径，比如：models&gt;</p></pre></section><ul><li><section><span>方案二，ModelScope 下载</span></section></li></ul><p>新建 <span>model_download.py</span> 文件，填入以下内容，替换整个 &lt;xxx&gt; <xxx>为你自</xxx><xxx>己的内容,保存后使用 </xxx><span>python model_download.py</span> 执行下载。　</p><section><p><br></p><pre><p><span>from</span> modelscope <span>import</span> snapshot_download<br>model_dir = snapshot_download(<span>'Qwen/Qwen2.5-3B-Instruct'</span>, cache_dir=<span>'&lt;你想要存放的路径，比如：models&gt;'</span>, revision=<span>'master'</span>)</p></pre></section><h2>编写配置文件和训练代码</h2><p>接下来我们需要准备 3 个文件，我们会在 <span>Unlock-DeepSeek</span><span>（https://github.com/datawhalechina/unlock-deepseek）</span> 项目中提供完整的复现文件，方便同学们直接使用。　</p><ul><li><section><span>第一个是 Accelerate 配置文件，用于分布式训练（三张卡）。新建 </span><span>deepspeed_zero3.yaml</span><span> 填入以下内容并保存（不是 DeepSeek，别看错！）。</span></section></li></ul><section><p><br></p><pre><p><span>compute_environment:</span> <span>LOCAL_MACHINE</span><br><span>debug:</span> <span>false</span><br><span>deepspeed_config:</span><br>  <span>deepspeed_multinode_launcher:</span> <span>standard</span><br>  <span>offload_optimizer_device:</span> <span>none</span><br>  <span>offload_param_device:</span> <span>none</span><br>  <span>zero3_init_flag:</span> <span>true</span><br>  <span>zero3_save_16bit_model:</span> <span>true</span><br>  <span>zero_stage:</span> <span>3</span><br><span>distributed_type:</span> <span>DEEPSPEED</span><br><span>downcast_bf16:</span> <span>'no'</span><br><span>machine_rank:</span> <span>0</span><br><span>main_training_function:</span> <span>main</span><br><span>mixed_precision:</span> <span>bf16</span><br><span>num_machines:</span> <span>1</span><br><span>num_processes:</span> <span>8</span> <span># 我们在这里保持常规默认的 8 卡机器，会在后面的启动命令中覆盖新值</span><br><span>rdzv_backend:</span> <span>static</span><br><span>same_network:</span> <span>true</span><br><span>tpu_env:</span> []<br><span>tpu_use_cluster:</span> <span>false</span><br><span>tpu_use_sudo:</span> <span>false</span><br><span>use_cpu:</span> <span>false</span></p></pre></section><p>一般来说，这个文件内容不需要修改，如果有定制需求，请不要使用这个文件，运行 <span>accelerate config</span> 自行设定。　</p><p>在介绍下一个文件之前，我们强烈建议大家使用 <span>Swanlab</span><span>（https://swanlab.cn/）</span> 来可视化追踪实验过程，打开：https://swanlab.cn/login ，登录之后点击图中所示的 Quick Start，或者打开：https://swanlab.cn/space/~/settings ，复制 API Key。　</p><p><img data-imgfileid="100216633" data-ratio="1.0731481481481482" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUSde5Fd9qoBMXpQykhwicfjibJ1PvmrBQWKtu729U10tlTjPNqE33ENHg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUSde5Fd9qoBMXpQykhwicfjibJ1PvmrBQWKtu729U10tlTjPNqE33ENHg/640?wx_fmt=png&amp;from=appmsg"></p><p><img data-imgfileid="100216631" data-ratio="0.5425925925925926" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU2Cb5atQYuNOaoV9Dxc1Yv8w3NSKaXEmkzdlFib7AtLaEcvvL1fkXPCA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU2Cb5atQYuNOaoV9Dxc1Yv8w3NSKaXEmkzdlFib7AtLaEcvvL1fkXPCA/640?wx_fmt=png&amp;from=appmsg"></p><section>在终端输入<span>swanlab login</span>，直接粘贴（你是看不见东西被粘贴上去的），回车，出现类似如下提示就是登录成功。　</section><section><img data-imgfileid="100216630" data-ratio="0.13333333333333333" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU8tRuCP9yX4tMbrez1Q7VxYkrfrFl3XBdvJj4f5KJicyFmH7J71YYx0A/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU8tRuCP9yX4tMbrez1Q7VxYkrfrFl3XBdvJj4f5KJicyFmH7J71YYx0A/640?wx_fmt=png&amp;from=appmsg"></section><ul><li><section><span>第二个是 TRL 配置文件，在这里我们会设定训练的超参数。新建 </span><span>Datawhale-R1.yaml</span><span> 填入以下内容，并根据实际情况修改</span><span>（阅读注释）</span><span>，并保存。</span></section></li></ul><section><section><br></section><pre><section><span># 模型参数</span><br><span>model_name_or_path:</span> <span>&lt;你的模型存放的路径，比如：models/Qwen/Qwen2.5-3B-Instruct&gt;</span><br><span>model_revision:</span> <span>main</span><br><span>torch_dtype:</span> <span>bfloat16</span><br><span>attn_implementation:</span> <span>flash_attention_2</span><br><span>bf16:</span> <span>true</span><br><span>tf32:</span> <span>true</span><br><span>output_dir:</span> <span>&lt;你想要模型输出的路径，比如</span> <span>output/Datawhale-R1&gt;</span><br><br><span># 数据集参数</span><br><span>dataset_id_or_path:</span> <span>&lt;你的数据集存放的路径，比如：dataset&gt;</span><br><br><span># Swanlab 训练流程记录参数</span><br><span>swanlab:</span> <span>true</span> <span># 是否开启 Swanlab </span><br><span>workspace:</span> <span>&lt;用户名&gt;</span><br><span>project:</span> <span>&lt;项目名，整个复现项目的名称，例如：Datawhale-R1-by_xxx&gt;</span><br><span>experiment_name:</span> <span>&lt;实验名，某次超参数运行的自定义名称，例如：qwen2.5-3B-lr:5e-7_beta:0.001&gt;</span><br><br><span># 训练参数</span><br><span>max_steps:</span> <span>450</span> <span># 最大训练步长</span><br><span>per_device_train_batch_size:</span> <span>1</span><br><span>gradient_accumulation_steps:</span> <span>8</span><br><span>gradient_checkpointing:</span> <span>true</span><br><span>gradient_checkpointing_kwargs:</span><br>  <span>use_reentrant:</span> <span>false</span><br><span>learning_rate:</span> <span>5.0e-7</span> <span># 学习率，调整过，参见下文介绍</span><br><span>lr_scheduler_type:</span> <span>cosine</span> <span># 学习率衰减方案</span><br><span>warmup_ratio:</span> <span>0.03</span> <span># 学习率预热比率（对于整个步长），好用！</span><br><span>seed:</span> <span>2025</span> <span># 随机种子，方便实验复现</span><br><br><span># GRPO 算法参数</span><br><span>beta:</span> <span>0.001</span> <span># KL 惩罚因子，调整过，参见下文介绍</span><br><span>max_prompt_length:</span> <span>256</span> <span># 输入 prompt 最大长度，本实验基本不会有太大变化</span><br><span>max_completion_length:</span> <span>4096</span> <span># 输出回答长度，包含推理思维链，设为 4K 比较合适</span><br><span>num_generations:</span> <span>8</span><br><span>use_vllm:</span> <span>true</span> <span># 启用 vllm 来加速推理</span><br><span>vllm_device:</span> <span>&lt;计算卡编号，例如：cuda:2&gt;</span> <span># 留出一张卡来启用 vllm 推理，参见下文介绍</span><br><span>vllm_gpu_memory_utilization:</span> <span>0.5</span><br><br><span># Logging arguments</span><br><span>logging_strategy:</span> <span>steps</span><br><span>logging_steps:</span> <span>1</span><br><span>save_strategy:</span> <span>"steps"</span><br><span>save_steps:</span> <span>50</span> <span># 每隔多少步保存一次</span></section></pre></section><section>我们并没有介绍全部参数，如果需要调整，请查阅 Huggingface 相关文档。当然，直接询问 DeepSeek 可能是更快的方式。　</section><section>这份配置文件中有一些值得大家注意的地方：　</section><ul><li><section><span>learning_rate</span><span> 和 </span><span>beta</span><span> 在 GRPO 的原始论文</span><span>《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》</span><span>（https://arxiv.org/abs/2402.03300）</span><span>里分别为 </span><span>1e-6</span><span> 和 </span><span>0.04</span><span>。在这里我们根据</span><span>《Unraveling RLHF and Its Variants: Progress and Practical Engineering Insights》</span><span>（https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights）</span><span>将其调整为 </span><span>5e-7</span><span> 和 </span><span>0.001</span><span>。</span></section></li><li><section><span>vllm_device</span><span> 本实验需要留出一张卡作为 vllm 的推理卡，假设我们手上有 3 张卡（编号cuda: 0, cuda: 1, cuda: 2)，我们需要指定其中一张卡为 vllm 推理卡，例如我们指定最后一张 </span><span>cuda:2</span><span>。另外，如果你使用了</span><span>CUDA_VISIBLE_DEVICES</span><span> 情况会有些不一样，比如我们有 8 张卡（编号 cuda:0-7），指定编号为 1、2、3 的卡可见（</span><span>CUDA_VISIBLE_DEVICES=1,2,3</span><span>)，这时我们想指定最后一张卡为 vllm 推理卡，则是需要设置为 </span><span>cuda:2</span><span>，因为设置完可见性后，cuda:1 -&gt; cuda:0，cuda:2 -&gt; cuda:1，cuda:3 -&gt; cuda:2，所以原先的 3 号卡变为了新编号的 2 号卡。</span></section></li><li><section><span>save_steps</span><span> 在 </span><span>mini-r1</span><span>（https://www.philschmid.de/mini-deepseek-r1）</span><span> 中是被设为 </span><span>25</span><span>，但是跑完整个训练后，保存的文件大小达到了 700+ GB！因为不仅包含了模型，还包含了其他卡的优化器状态和其他检查点信息，我们在这里改为 </span><span>50</span><span>，但仍然要提醒同学们设置成合适自己的大小（训练代码中已经包含结束后保存模型的代码）。</span></section></li><li><section><span>最后，就是创建训练代码文件 </span><span>train_Datawhale-R1.py</span><span> 并保存，我们几乎给每个关键步骤都添加了注释（建议大家从后往前读），在后文我们会再梳理一遍核心步骤。</span></section></li></ul><section><section><br></section><pre><section><span>import</span> logging<br><span>import</span> os<br><span>import</span> random<br><span>import</span> re<br><span>from</span> dataclasses <span>import</span> dataclass<br><span>from</span> datetime <span>import</span> datetime<br><span>from</span> typing <span>import</span> <span>List</span><br><br><span>from</span> datasets <span>import</span> load_dataset<br><span>from</span> swanlab.integration.transformers <span>import</span> SwanLabCallback<br><span>from</span> transformers <span>import</span> AutoTokenizer<br><span>from</span> transformers.trainer_utils <span>import</span> get_last_checkpoint<br><span>from</span> trl <span>import</span> GRPOConfig, GRPOTrainer, ModelConfig, TrlParser<br><br><span>@dataclass</span><br><span>class</span> <span>DatasetArguments</span>:<br>    <span>"""数据集参数的数据类"""</span><br><br>    <span># 数据集 ID 或路径</span><br>    dataset_id_or_path: <span>str</span> = <span>"Jiayi-Pan/Countdown-Tasks-3to4"</span><br>    <span># 数据集拆分</span><br>    dataset_splits: <span>str</span> = <span>"train"</span><br>    <span># 分词器名称或路径</span><br>    tokenizer_name_or_path: <span>str</span> = <span>None</span><br><br><span>@dataclass</span><br><span>class</span> <span>SwanlabArguments</span>:<br>    <span>"""SwanLab参数的数据类"""</span><br><br>    <span># 是否使用 SwanLab</span><br>    swanlab: <span>bool</span><br>    <span># SwanLab 用户名</span><br>    workspace: <span>str</span><br>    <span># SwanLab 的项目名</span><br>    project: <span>str</span><br>    <span># SwanLab 的实验名</span><br>    experiment_name: <span>str</span><br><br><span># 配置日志记录器</span><br>logging.basicConfig(level=logging.INFO)<br>logger = logging.getLogger(__name__)<br>logger.setLevel(logging.INFO)<br>handler = logging.StreamHandler()<br>handler.setFormatter(<br>    logging.Formatter(<span>"%(asctime)s - %(name)s - %(levelname)s - %(message)s"</span>)<br>)  <span># 设置日志格式</span><br><br>logger.addHandler(handler)<br><br><span>def</span> <span>format_reward_func</span>(completions, **kwargs):<br>    <span>"""<br>    格式奖励函数，检查模型输出格式是否匹配: &lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;<br><br>    参数:<br>        completions (list[str]): 生成的输出<br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出</span><br>    <span>for</span> completion <span>in</span> completions:<br>        <span>try</span>:<br>            <span># 在生成的输出前添加&lt;think&gt;标签，便于后续正则表达式匹配</span><br>            completion = <span>"&lt;think&gt;"</span> + completion<br><br>            <span>if</span> random.random() &lt; <span>0.1</span>:  <span># 1% 的概率将生成输出写入文件</span><br>                <span># 创建生成输出目录（如果不存在）</span><br>                os.makedirs(<span>"completion_samples"</span>, exist_ok=<span>True</span>)<br>                log_file = os.path.join(<span>"completion_samples"</span>, <span>"completion_samples.txt"</span>)<br>                <span>with</span> <span>open</span>(log_file, <span>"a"</span>) <span>as</span> f:<br>                    f.write(<span>f"\n\n==============\n"</span>)<br>                    f.write(completion)  <span># 写入生成的输出</span><br><br>            <span># 定义正则表达式模式，用于匹配 &lt;think&gt; 和 &lt;answer&gt; 标签</span><br>            regex = <span>r"^&lt;think&gt;([^&lt;]*(?:&lt;(?!/?think&gt;)[^&lt;]*)*)&lt;\/think&gt;\n&lt;answer&gt;([\s\S]*?)&lt;\/answer&gt;$"</span><br>            <span>match</span> = re.search(regex, completion, re.DOTALL)  <span># 使用正则表达式进行匹配</span><br><br>            <span>if</span> <span>match</span> <span>is</span> <span>None</span> <span>or</span> <span>len</span>(<span>match</span>.groups()) != <span>2</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果格式不正确，奖励为 0</span><br>            <span>else</span>:<br>                rewards.append(<span>1.0</span>)  <span># 如果格式正确，奖励为 1</span><br>        <span>except</span> Exception:<br>            rewards.append(<span>0.0</span>)  <span># 如果发生异常，奖励为 0</span><br><br>    <span>return</span> rewards<br><br><span>def</span> <span>equation_reward_func</span>(completions, target, nums, **kwargs):<br>    <span>"""<br>    方程奖励函数，检查计算结果是否正确，数字是否符合使用要求（每个数字只用一次，只使用所提供的数字）<br><br>    参数:<br>        completions (list[str]): 生成的输出<br>        target (list[str]): 预期的答案<br>        nums (list[str]): 可用的数字<br><br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出、预期的答案和可用的数字</span><br>    <span>for</span> completion, gt, numbers <span>in</span> <span>zip</span>(completions, target, nums):<br>        <span>try</span>:<br>            <span># 在生成的输出前添加 &lt;think&gt; 标签，便于后续正则表达式匹配</span><br>            completion = <span>"&lt;think&gt;"</span> + completion<br>            <span># 定义正则表达式模式，用于匹配 &lt;answer&gt; 标签</span><br>            <span>match</span> = re.search(<span>r"&lt;answer&gt;(.*?)&lt;\/answer&gt;"</span>, completion)<br>            <span>if</span> <span>match</span> <span>is</span> <span>None</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果没有匹配到 &lt;answer&gt; 标签，奖励为 0</span><br>                <span>continue</span><br>            equation = <span>match</span>.group(<span>1</span>).strip()  <span># 提取 &lt;answer&gt; 标签中的内容</span><br>            <span># 提取方程中的所有数字</span><br>            used_numbers = [<span>int</span>(n) <span>for</span> n <span>in</span> re.findall(<span>r"\d+"</span>, equation)]<br><br>            <span># 检查所有数字是否被使用且只使用一次</span><br>            <span>if</span> <span>sorted</span>(used_numbers) != <span>sorted</span>(numbers):<br>                rewards.append(<span>0.0</span>)<br>                <span>continue</span><br><br>            <span># 定义允许的字符模式，只允许数字、运算符、括号和空白字符</span><br>            allowed_pattern = <span>r"^[\d+\-*/().\s]+$"</span><br>            <span>if</span> <span>not</span> re.<span>match</span>(allowed_pattern, equation):<br>                rewards.append(<span>0.0</span>)  <span># 如果方程包含不允许的字符，奖励为 0</span><br>                <span>continue</span><br><br>            <span># 计算方程的结果</span><br>            result = <span>eval</span>(equation, {<span>"__builtins__"</span>: <span>None</span>}, {})<br>            <span># 检查方程是否正确且与预期答案匹配（误差小于 1e-5）</span><br>            <span>if</span> <span>abs</span>(<span>float</span>(result) - <span>float</span>(gt)) &lt; <span>1e-5</span>:<br>                rewards.append(<span>1.0</span>)  <span># 如果正确，奖励为 1</span><br><br>                <span># 10% 的概率将成功的样本写入文件</span><br>                <span>if</span> random.random() &lt; <span>0.10</span>:<br>                    <span># 创建生成输出目录（如果不存在）</span><br>                    os.makedirs(<span>"completion_samples"</span>, exist_ok=<span>True</span>)<br>                    log_file = os.path.join(<br>                        <span>"completion_samples"</span>, <span>"success_completion_samples.txt"</span><br>                    )<br>                    <span>with</span> <span>open</span>(log_file, <span>"a"</span>) <span>as</span> f:<br>                        f.write(<span>f"\n\n==============\n"</span>)<br>                        f.write(completion)  <span># 写入生成的输出</span><br>            <span>else</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果不正确，奖励为 0</span><br>        <span>except</span> Exception:<br>            rewards.append(<span>0.0</span>)  <span># 如果评估失败，奖励为 0</span><br><br>    <span>return</span> rewards<br><br><span>def</span> <span>thought_len_reward_func</span>(completions, **kwargs):<br>    <span>"""<br>    思考长度奖励函数，检查 &lt;think&gt; 标签的长度是否大于 1000<br><br>    参数:<br>        completions (list[str]): 生成的输出<br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出</span><br>    <span>for</span> completion <span>in</span> completions:<br>        <span>try</span>:<br>            <span># 在生成的输出前添加 &lt;think&gt; 标签，便于后续正则表达式匹配</span><br>            completion = <span>"&lt;think&gt;"</span> + completion<br>            <span># 定义正则表达式模式，用于匹配 &lt;think&gt; 标签</span><br>            <span>match</span> = re.search(<span>r"&lt;think&gt;(.*?)&lt;/think&gt;"</span>, completion)<br>            <span># 如果匹配到 &lt;think&gt; 标签</span><br>            <span>if</span> <span>match</span>:<br>                thought_process = <span>match</span>.group(<span>1</span>).strip()  <span># 提取 &lt;think&gt; 标签中的内容</span><br>                thought_length = <span>len</span>(thought_process)  <span># 计算思考过程的长度</span><br>                <span>if</span> thought_length &gt; <span>1000</span>:<br>                    rewards.append(<span>1.0</span>)  <span># 如果思考过程长度大于 1000，奖励为 1</span><br>                <span>else</span>:<br>                    rewards.append(<span>0.0</span>)  <span># 否则奖励为 0</span><br>            <span>else</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果没有匹配到 &lt;think&gt; 标签，奖励为 0</span><br>                <span>continue</span><br>        <span>except</span> Exception:<br>            rewards.append(<span>0.0</span>)  <span># 如果发生异常，奖励为 0</span><br><br>    <span>return</span> rewards<br><br><span>def</span> <span>get_checkpoint</span>(training_args: GRPOConfig):<br>    <span>"""<br>    获取最后一个检查点<br><br>    参数:<br>        training_args (GRPOConfig): 训练参数<br>    返回:<br>        str: 最后一个检查点的路径，如果没有检查点，则返回 None<br>    """</span><br>    last_checkpoint = <span>None</span><br>    <span>if</span> os.path.isdir(training_args.output_dir):  <span># 如果输出目录存在</span><br>        <span># 获取最后一个检查点</span><br>        last_checkpoint = get_last_checkpoint(training_args.output_dir)<br>    <span>return</span> last_checkpoint<br><br><span># 定义 GRPO 训练函数</span><br><span>def</span> <span>grpo_function</span>(<br>    model_args: ModelConfig,<br>    dataset_args: DatasetArguments,<br>    training_args: GRPOConfig,<br>    callbacks: <span>List</span>,<br>):<br>    <span># 记录模型参数</span><br>    logger.info(<span>f"Model parameters <span>{model_args}</span>"</span>)<br>    <span># 记录训练/评估参数</span><br>    logger.info(<span>f"Training/evaluation parameters <span>{training_args}</span>"</span>)<br><br>    <span># 加载分词器</span><br>    tokenizer = AutoTokenizer.from_pretrained(<br>        (<br>            <span># 如果有指定分词器，则使用指定的分词器，否则使用模型名称</span><br>            dataset_args.tokenizer_name_or_path<br>            <span>if</span> dataset_args.tokenizer_name_or_path<br>            <span>else</span> model_args.model_name_or_path<br>        ),<br>        revision=model_args.model_revision,  <span># 使用指定的模型版本</span><br>        trust_remote_code=model_args.trust_remote_code,  <span># 允许使用远程代码</span><br>    )<br>    <span># 如果分词器没有填充标记，则使用结束标记作为填充标记</span><br>    <span>if</span> tokenizer.pad_token <span>is</span> <span>None</span>:<br>        tokenizer.pad_token = tokenizer.eos_token<br><br>    <span># 加载数据集</span><br>    dataset = load_dataset(<br>        dataset_args.dataset_id_or_path, split=dataset_args.dataset_splits<br>    )<br>    <span># 随机选择 50K 个样本，看你喜好定数字，但是数据集有 409K 个样本</span><br>    dataset = dataset.shuffle(seed=training_args.seed).select(<span>range</span>(<span>50000</span>))<br><br>    <span>def</span> <span>generate_r1_prompt</span>(numbers, target):<br>        <span>"""<br>        生成 R1 Countdown 游戏提示词<br><br>        参数:<br>            numbers (list[int]): 数字列表<br>            target (int): 目标值<br>        返回:<br>            dict: 生成的一个数据样本<br>        """</span><br>        <span># 定义提示词前缀</span><br>        r1_prefix = [<br>            {<br>                <span>"role"</span>: <span>"user"</span>,<br>                <span>"content"</span>: <span>f"使用给定的数字 <span>{numbers}</span>，创建一个等于 <span>{target}</span> 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。"</span>,<br>            },<br>            {<br>                <span>"role"</span>: <span>"assistant"</span>,<br>                <span>"content"</span>: <span>"让我们逐步解决这个问题。\n&lt;think&gt;"</span>,  <span># 结尾使用 `&lt;think&gt;` 促使模型开始思考</span><br>            },<br>        ]<br><br>        <span>return</span> {<br>            <span>"prompt"</span>: tokenizer.apply_chat_template(<br>                r1_prefix, tokenize=<span>False</span>, continue_final_message=<span>True</span><br>            ),  <span># 提示词，continue_final_message=True 表示将提示词中的最后一个消息继续到最终的输出中</span><br>            <span>"target"</span>: target,<br>            <span>"nums"</span>: numbers,<br>        }<br><br>    <span># 将数据集转换为 R1 Countdown 游戏提示词</span><br>    dataset = dataset.<span>map</span>(<span>lambda</span> x: generate_r1_prompt(x[<span>"nums"</span>], x[<span>"target"</span>]))<br>    <span># 将数据集拆分为训练集和测试集，拆分比例为 9:1</span><br>    train_test_split = dataset.train_test_split(test_size=<span>0.1</span>)<br>    train_dataset = train_test_split[<span>"train"</span>]  <span># 获取训练集</span><br>    test_dataset = train_test_split[<span>"test"</span>]  <span># 获取测试集</span><br><br>    <span># 设置 GRPOTrainer</span><br>    trainer = GRPOTrainer(<br>        model=model_args.model_name_or_path,  <span># 模型名称或路径</span><br>        <span># 奖励函数列表，用于计算奖励分数</span><br>        reward_funcs=[<br>            format_reward_func,  <span># 格式奖励函数</span><br>            equation_reward_func,  <span># 方程奖励函数</span><br>            thought_len_reward_func,  <span># 思考长度奖励函数</span><br>        ],<br>        args=training_args,<br>        train_dataset=train_dataset,<br>        eval_dataset=test_dataset,<br>        callbacks=callbacks,<br>    )<br><br>    last_checkpoint = get_checkpoint(training_args)  <span># 检查最后一个检查点</span><br>    <span># 如果检测到检查点且指定从检查点恢复训练，则记录信息</span><br>    <span>if</span> last_checkpoint <span>is</span> <span>not</span> <span>None</span> <span>and</span> training_args.resume_from_checkpoint <span>is</span> <span>None</span>:<br>        logger.info(<span>f"Checkpoint detected, resuming training at <span>{last_checkpoint}</span>."</span>)<br><br>    logger.info(<br>        <span>f'*** Starting training <span>{datetime.now().strftime(<span>"%Y-%m-%d %H:%M:%S"</span>)}</span> for <span>{training_args.num_train_epochs}</span> epochs***'</span><br>    )<br><br>    <span># 训练模型</span><br>    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)<br><br>    <span># 记录和保存指标</span><br>    metrics = train_result.metrics<br>    metrics[<span>"train_samples"</span>] = <span>len</span>(train_dataset)<br>    trainer.log_metrics(<span>"train"</span>, metrics)<br>    trainer.save_metrics(<span>"train"</span>, metrics)<br>    trainer.save_state()<br><br>    logger.info(<span>"*** Training complete ***"</span>)<br><br>    <span># 保存模型和分词器</span><br>    logger.info(<span>"*** Save model ***"</span>)<br>    trainer.model.config.use_cache = <span>True</span><br>    trainer.save_model(training_args.output_dir)<br>    logger.info(<span>f"Model saved to <span>{training_args.output_dir}</span>"</span>)<br>    training_args.distributed_state.wait_for_everyone()  <span># 等待所有进程加载</span><br>    tokenizer.save_pretrained(training_args.output_dir)<br>    logger.info(<span>f"Tokenizer saved to <span>{training_args.output_dir}</span>"</span>)<br><br>    logger.info(<span>"*** Training complete! ***"</span>)<br><br><span>def</span> <span>main</span>():<br>    <span>"""主函数，用于执行主训练循环"""</span><br>    <span># 解析命令行参数和配置文件</span><br>    parser = TrlParser((ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments))<br>    model_args, dataset_args, training_args, swanlab_args = (<br>        parser.parse_args_and_config()<br>    )<br><br>    <span># 如果使用 SwanLab，则创建 SwanLab 回调对象，用于训练信息记录</span><br>    <span>if</span> swanlab_args.swanlab:<br>        swanlab_callback = SwanLabCallback(<br>            workspace=swanlab_args.workspace,<br>            project=swanlab_args.project,<br>            experiment_name=swanlab_args.experiment_name,<br>        )<br>        callbacks = [swanlab_callback]<br>    <span>else</span>:<br>        callbacks = <span>None</span><br><br>    <span># 运行主训练循环</span><br>    grpo_function(model_args, dataset_args, training_args, callbacks=callbacks)<br><br><span>if</span> __name__ == <span>"__main__"</span>:<br>    main()<br></section></pre></section><h1>启动训练</h1><section>肯定有一些同学已经等不及要开始跑模型训练了，那启动训练的命令很简单，在终端运行下面的内容（<span>根据自己需求修改</span>），也可以把它保存为 <span>train_Datawhale-R1.sh</span> 然后在终端运行 <span>bash train_Datawhale-R1.sh</span>。　</section><section><section><br></section><pre><section><span># 如果你要限制计算卡编号，请在这里设置，例如只使用 cuda:1-3，如果不用限制，就删除下面这行</span><br><span>export</span> CUDA_VISIBLE_DEVICES=1,2,3<br><br>accelerate launch \<br>    --num_processes 2 \<br>    --config_file deepspeed_zero3.yaml \<br>    train_Datawhale-R1.py \<br>    --config Datawhale-R1.yaml</section></pre></section><section>注意：<span>--num_processes</span> 是由你希望使用的计算卡数量决定，我们之前在配置文件那里说过，要留一张卡作为 vllm 的推理卡，那么 <span>--num_processes</span> 的数值应该是你要使用的计算卡数量 <span>n-1</span>，例如我有 3 张卡，我的 <span>--num_processes</span> 应该为 <span>2</span>。这里的 <span>--num_processes</span> 的数值也会把 <span>deepspeed_zero3.yaml</span> 的<span>num_processes</span> 设置的 <span>8</span> 给覆盖掉。　</section><section>另外，同样像上文所说，如果你有定制的硬件配置需求，请不要使用 <span>--config_file</span> 参数。　</section><section>出现这样的提示就说明模型已经训练起来啦！可以在 Swanlab 看炫酷的训练数据了（手机也能看，特别适合天选炼丹人）。　</section><section><img data-imgfileid="100216638" data-ratio="0.2564814814814815" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdnIupEnf5xXEjsE8ZTKib7ib1bTghUR49LiaBIAibt7viaEsjRrpWCYy8fw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdnIupEnf5xXEjsE8ZTKib7ib1bTghUR49LiaBIAibt7viaEsjRrpWCYy8fw/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216639" data-ratio="0.55" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUpH036RITNuo9Ou5qCia6MQK8owokzibic01ATvVEhLRbkVJzAeGZX2WCA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUpH036RITNuo9Ou5qCia6MQK8owokzibic01ATvVEhLRbkVJzAeGZX2WCA/640?wx_fmt=png&amp;from=appmsg"></section><h1>训练流程详解</h1><h2>流程总览</h2><section>我们来梳理一遍 Datawhale-R1 训练流程：　</section><ol><li><section><span>将提示词输入到 Qwen 2.5 模型。</span></section></li><li><section><span>Qwen 2.5 输出多个带思考的回答（本实验设置为 8，由 </span><span>num_generations</span><span> 参数决定）。</span></section></li><li><section><span>模型的回答分别传入三个奖励函数计算，计算的结果相加。</span></section></li><li><section><span>将奖励值传入 GRPO 策略中，GRPO 根据奖励值来决定如何调整 Qwen 2.5 模型。</span></section></li><li><section><span>重复上述流程（本实验重复了 450 次，由 </span><span>max_steps</span><span> 参数决定）。</span></section></li></ol><p><img data-galleryid="" data-imgfileid="100216659" data-ratio="0.5916666666666667" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUYqHKZgiamzIWibWrPG8jnbM6f1Xu3yqrD1SRFYzcoWaOXZVgiaSgLsA4w/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUYqHKZgiamzIWibWrPG8jnbM6f1Xu3yqrD1SRFYzcoWaOXZVgiaSgLsA4w/640?wx_fmt=png"></p><section>有些同学可能不太熟悉强化学习，我们会在后续其他的文章中介绍强化学习相关的概念。在这里我们用一个例子来比喻一下：我们现在假设有一所学校，里面有一个数学老师（GRPO 策略），还有一个班级（Qwen 2.5 模型，我们假设班级中的所有同学能力相同），学校每个月要月考（多步），每次月考是班级根据试卷（提示词，一份试卷只有一道题）写出多份答卷（班级有多个同学，所以会有多份答卷，对应多个带思考的模型回答，这些回答不一定是相同的），这时候数学老师就要去批改这些答卷（奖励函数计算），评卷规则是：　</section><ol><li><section><span>检查答题格式是否规范（格式奖励函数）</span></section></li><li><section><span>解题结果是否正确（方程奖励函数）</span></section></li><li><section><span>解题步骤是否详细（思考长度奖励函数）</span></section></li></ol><section>最后，把每部分的分数相加，得到多个试卷分数（多个奖励值，用 Python 列表表示，每个回答都对应一个奖励值），数学老师根据班级的月考分数来判断下一步如何调整教学计划（调整模型），来让这个班级在下一次月考中尽可能得到更高的分数。　</section><section>如果我们说得更细致一点，其实是数学老师会教整个班级“看到什么之后写什么”，比如看到题目就要写“解：”，看到“x+1=2”就要写“解得：x=1”，力求让组成回答的每一个字都是最合适的（位置要合适，用词也要合适）从而去获得最高的分数。　</section><section>这里的思考长度奖励函数是我们新加入的，用于鼓励模型进行更长的思考。所以我们应该有个朴素的感受，随着训练的不断进行，Datawhale-R1 的输出格式应该会越来越规范，正确率也会不断提高，思考的长度也会增加。　</section><h2>核心代码介绍</h2><section>我们稍微介绍一下代码中每个核心步骤的输入输出样例，让大家心里有个底。首先是各种 <span>xxx_args</span> 参数，它其实就是根据下面这行代码，去获取我们传入的 <span>Datawhale-R1.yaml</span> 里面的参数。　</section><section><section><br></section><pre><section>parser = TrlParser((ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments))<br>model_args, dataset_args, training_args, swanlab_args = (<br>    parser.parse_args_and_config()<br>)</section></pre></section><section>你可以看到我们定义了一个 <span>SwanlabArguments</span> 类，<span>TrlParser</span> 会去寻找 <span>Datawhale-R1.yaml</span> 中跟 <span>SwanlabArguments</span> 有关的参数，并把它赋值给 <span>swanlab_args</span>，由于每个参数名被要求是唯一的，不能重复，所以 <span>TrlParser</span> 能把不同的参数正确赋值给对应变量（根据 <span>ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments</span> 的顺序，赋值给 <span>model_args, dataset_args, training_args, swanlab_args</span>）　</section><section><section><br></section><pre><section><span># train_Datawhale-R1.py</span><br><br><span>@dataclass</span><br><span>class</span> <span>SwanlabArguments</span>:<br>    <span>"""SwanLab参数的数据类"""</span><br><br>    <span># 是否使用 SwanLab</span><br>    swanlab: <span>bool</span><br>    <span># SwanLab 用户名</span><br>    workspace: <span>str</span><br>    <span># SwanLab 的项目名</span><br>    project: <span>str</span><br>    <span># SwanLab 的实验名</span><br>    experiment_name: <span>str</span></section></pre></section><section><section><br></section><pre><section><span># Datawhale-R1.yaml</span><br><br><span># Swanlab 训练流程记录参数</span><br><span>swanlab:</span> <span>true</span> <span># 是否开启 Swanlab </span><br><span>workspace:</span> <span>&lt;用户名&gt;</span><br><span>project:</span> <span>&lt;项目名，整个复现项目的名称，例如：Datawhale-R1-by_xxx&gt;</span><br><span>experiment_name:</span> <span>&lt;实验名，某次超参数运行的自定义名称，例如：qwen2.5-3B-lr:5e-7_beta:0.001&gt;</span></section></pre></section><section>接下来就到了 <span>grpo_function</span> 里，我们首先来看看我们的数据集长什么样子，我们的任务其实很简单，它很像 24 点游戏，给定若干个数字 <span>nums</span>，例如 <span>[44, 19, 35]</span> ，模型要用四则运算，告诉我们一个方程，它的计算结果正好是 <span>target</span>，例如 <span>98</span>，详细要求我们在 prompt 中给大家展示。　</section><section><img data-imgfileid="100216635" data-ratio="1.1489637305699483" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUibTJYA3DGlnpK1Cx3cwa9icoOz0D8elQzX7ibe209jQL3tu7Wahicdsdjg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="772" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUibTJYA3DGlnpK1Cx3cwa9icoOz0D8elQzX7ibe209jQL3tu7Wahicdsdjg/640?wx_fmt=png&amp;from=appmsg"></section><section>然后我们的 prompt 如下，利用 Python 的 f-strings 功能来填入具体数值，并且在 <span>assistant</span> 的结尾加入了 <span>\n&lt;think&gt;</span>，来促使我们的模型开始按要求逐步思考。提示词是用 DeepSeek 翻译的 mini-r1 的提示词，咱们中国人阅读中文的速度更快些。　</section><section><section><br></section><pre><section>r1_prefix = [<br>    {<br>        <span>"role"</span>: <span>"user"</span>,<br>        <span>"content"</span>: <span>f"使用给定的数字 <span>{numbers}</span>，创建一个等于 <span>{target}</span> 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。"</span>,<br>    },<br>    {<br>        <span>"role"</span>: <span>"assistant"</span>,<br>        <span>"content"</span>: <span>"让我们逐步解决这个问题。\n&lt;think&gt;"</span>,  <span># 结尾使用 `&lt;think&gt;` 促使模型开始思考</span><br>    },<br>]</section></pre></section><section>在这里我们会把 prompt 转换为 Qwen 2.5 的提示词模版，让它以更熟悉的方式来接收提示词，并且我们把 <span>让我们逐步解决这个问题。\n<span>&lt;think&gt;</span><think></think></span> 作为模型输出的开头，让它接着续写。用 Python 字典的方式返回样本，这样 TRL 会在调用奖励函数的时候，帮我们把键名设为为对应的参数；另外，TRL 会把模型的多个输出设为 <span>completions</span>。　</section><section><section><br></section><pre><section><span>return</span> {<br>    <span>"prompt"</span>: tokenizer.apply_chat_template(<br>        r1_prefix, tokenize=<span>False</span>, continue_final_message=<span>True</span><br>    ),  <span># 提示词，continue_final_message=True 表示将提示词中的最后一个消息继续到最终的输出中</span><br>    <span>"target"</span>: target,<br>    <span>"nums"</span>: numbers,<br>}</section></pre></section><section><span>map</span> 方法会帮我们把实际的 <span>nums</span> 和 <span>target</span> 填入到 prompt 里，我们根据上面举的例子，来看一个具体的提示词：　</section><section><section><br></section><pre><section><span># 将数据集转换为 R1 Countdown 游戏提示词</span><br>dataset = dataset.<span>map</span>(<span>lambda</span> x: generate_r1_prompt(x[<span>"nums"</span>], x[<span>"target"</span>]))<br><br><span># 举例</span><br>nums = [<span>44</span>, <span>19</span>, <span>35</span>]<br>target = <span>98</span><br>r1_prefix = {<br>    <span>"role"</span>: <span>"user"</span>,<br>    <span>"content"</span>: <span>f"使用给定的数字 [44, 19, 35]，创建一个等于 98 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。"</span>,<br>},<br>{<br>    <span>"role"</span>: <span>"assistant"</span>,<br>    <span>"content"</span>: <span>"让我们逐步解决这个问题。\n&lt;think&gt;"</span>,  <span># 结尾使用 `&lt;think&gt;` 促使模型开始思考</span><br>},<br><br><span># 转换为 Qwen 提示词模版后</span><br>prompt = <span>"&lt;|im_start|&gt;system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n使用给定的数字 [44, 19, 35]，创建一个等于 98 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n让我们逐步解决这个问题。\n&lt;think&gt;"</span> <span># 模型将在 \n&lt;think&gt; 后续写</span></section></pre></section><section>我们最后来看一个奖励函数的例子，TRL 将多个模型输出变成一个列表，叫做 <span>completions</span>，并将数据集中的其他内容根据键名传入到对应参数。所以我们需要使用 for 循环遍历所有的 <span>completions</span>，并对每个输出进行判断打分，最后返回每个输出的得分列表 <span>reward</span> 给 <span>GRPO 策略</span>（例如：[0.0, 1.0, 0.0]），让其判断下一步如何调整。</section><section><section><br></section><pre><section><span>def</span> <span>equation_reward_func</span>(completions, target, nums, **kwargs):<br>    <span>"""<br>    参数:<br>        completions (list[str]): 生成的输出<br>        target (list[str]): 预期的答案<br>        nums (list[str]): 可用的数字<br><br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出、预期的答案和可用的数字</span><br>    <span>for</span> completion, gt, numbers <span>in</span> <span>zip</span>(completions, target, nums):<br>        ... <span># 进行一些 rewards.append() 操作</span><br>    <span>return</span> rewards</section></pre></section><section>OK，我们对复现流程的介绍就大致结束了，我们会在文末提供完整的文档，开源我们的复现工作。　</section><h1>训练结果解读</h1><section>现在我们来看看模型表现出了什么有意思的现象，提前声明，这不是严谨的科学研究，会有很多分析漏洞。首先我们使用了学习率预热和学习率衰减，在训练前期学习率都很大，后期慢慢衰减下来。　</section><section><img data-imgfileid="100216637" data-ratio="0.48703703703703705" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUoA4RPxXzd73gPDkEHpfTZ0GJD17ePHhtTVaycg0iahwxjyysUlaljjw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUoA4RPxXzd73gPDkEHpfTZ0GJD17ePHhtTVaycg0iahwxjyysUlaljjw/640?wx_fmt=png&amp;from=appmsg"></section><section>对比下面两张图，我们发现模型前期学习输出格式的速度很快，大概 20 到 30 步就能学得很好。但是后来由于我们的思考长度奖励函数，模型的输出长度被拉长，发生严重的重复现象，导致超出 4096 的输出被截断，格式不完整，格式奖励函数的奖励值就大幅下降，后面模型又开始缩短输出，稳定在 300 到 400，又恢复到正确格式。　</section><section><img data-imgfileid="100216636" data-ratio="0.47962962962962963" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUlxo8X4G1bIGeNcMf8kBbNMN5enXXKl7R0zbWZvB9ppTh89WEg1V35Q/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUlxo8X4G1bIGeNcMf8kBbNMN5enXXKl7R0zbWZvB9ppTh89WEg1V35Q/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216640" data-ratio="0.4962962962962963" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeq20bITdU0Hq0FAPiaK01J86zdFABHklf6XqDGz63cDibPqscWzWUkQA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeq20bITdU0Hq0FAPiaK01J86zdFABHklf6XqDGz63cDibPqscWzWUkQA/640?wx_fmt=png&amp;from=appmsg"></section><section>模型的不断重复输出看着其实挺可怕的，Visual Studio Code 会匹配相同字符并高亮，大家可以看看，右侧红框的缩略图几乎都是重复的回答。　</section><section><img data-imgfileid="100216644" data-ratio="0.6333333333333333" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdunVjvKeFib0rDtKwyicefdmFBtrymRybrNZkbiaTDCMI6VxZmANcVfwA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdunVjvKeFib0rDtKwyicefdmFBtrymRybrNZkbiaTDCMI6VxZmANcVfwA/640?wx_fmt=png&amp;from=appmsg"></section><section>我们发现，模型被鼓励拉长输出的时候，计算正确率也在提升，所以我们有个不严谨的判断，似乎拉长模型输出，能带一定的计算正确率的提升。观察下图可以发现，在 120 步时，模型的输出在越变越长，平均输出长度已经被拉到 400 左右，越来越多的输出已经超过 1000，方程计算正确率也在逐步升高，但是这时已经发生一些重复问题导致格式错误。　</section><section><img data-imgfileid="100216641" data-ratio="0.46111111111111114" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU5KKe8jzUpRc9rGI5kMd6G0WZl8ib09V7HIFdbicguYorUOrgGEJAHkFA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU5KKe8jzUpRc9rGI5kMd6G0WZl8ib09V7HIFdbicguYorUOrgGEJAHkFA/640?wx_fmt=png&amp;from=appmsg"></section><section>其实从上图我们也可以看到 GRPO 已经意识到重复问题带来的奖励值下降，它在 200 步左右开始逐步限制模型输出长度，而这时模型的计算正确率也保持在 0.3 到 0.4 左右。　</section><section>我们还发现，在训练初期，你会看到比较明显的方程奖励提升，而输出长度不断减小。模型似乎有一种趋向于缩短思考长度的趋势，所以我们引入思考长度奖励函数来对抗这种趋势，我们把它解释为模型计算能力提升之后，就像学霸一眼秒杀题目一样，模型不想输出更多“废话”来解释解题过程。　</section><section><img data-imgfileid="100216642" data-ratio="0.5898148148148148" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUuyuicOnwc2D1Uog7ibpoKLRGPddAMHAJrVhebftOHCGIt7UQqm1dqm1Q/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUuyuicOnwc2D1Uog7ibpoKLRGPddAMHAJrVhebftOHCGIt7UQqm1dqm1Q/640?wx_fmt=png&amp;from=appmsg"></section><section>在训练开始 1 分钟左右，我们就观察到下面的输出，还以为我们重现了 Aha Moment。后来证明其实不是，Qwen 2.5 很喜欢反复试错、验算，反复试错很容易导致上文提及的重复输出问题。　</section><section><img data-imgfileid="100216643" data-ratio="0.45092592592592595" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUsHgicdtgDzBTSJJqoE1Yktyich2OPfRcvzBFxZRWBtCjXOYbKiclDsxkA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUsHgicdtgDzBTSJJqoE1Yktyich2OPfRcvzBFxZRWBtCjXOYbKiclDsxkA/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216645" data-ratio="0.5120370370370371" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUqibeHjVXRDI7srfRXzCUIgV9Xiba5AMEf7J3mQSwbWmoribRypyYShJibw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUqibeHjVXRDI7srfRXzCUIgV9Xiba5AMEf7J3mQSwbWmoribRypyYShJibw/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216648" data-ratio="0.24814814814814815" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUjNBG3ibFHqLueBzeO675G8nQvYTicAw5JzXIKHuL02JlZYDPdbo791AA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUjNBG3ibFHqLueBzeO675G8nQvYTicAw5JzXIKHuL02JlZYDPdbo791AA/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216649" data-ratio="0.5064814814814815" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyURlIBk0UCqyXbficH3KeOacYCs0q7OosxfN9Wjo3cwZcz36r08xwJOGg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyURlIBk0UCqyXbficH3KeOacYCs0q7OosxfN9Wjo3cwZcz36r08xwJOGg/640?wx_fmt=png&amp;from=appmsg"></section><section>我们发现了另一种语言混用现象，哈哈哈。current n. 电流; adj. 当前的。　</section><section><img data-imgfileid="100216647" data-ratio="0.3398148148148148" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU71psKk71LrDQLq5pd3cexxTfylWu4YLHJE2jZuOqfUZyaxsDIKRIcg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU71psKk71LrDQLq5pd3cexxTfylWu4YLHJE2jZuOqfUZyaxsDIKRIcg/640?wx_fmt=png&amp;from=appmsg"></section><section>所以结论就是，我们没有复现 Aha Moment。其实在观察大量 Qwen 2.5 的输出之后，一种直觉告诉我，可能 Aha Moment 跟模型本身的输出风格相关，网友都说 DeepSeek 文风很锐利、很活泼，但是 Qwen 2.5 给人的感觉总是冷静、平和。简单做了一个不严谨的测试，可能能够佐证这个想法，我要求两个模型用 Aha Moment 的语气跟我说话，再随便回复了一个字，观察两个模型本身对 Aha Moment 的映射是会输出什么。我们另外测试了 Llama 和 MiniCPM，它们的输出风格都跟 Qwen 很接近，试图像说教一样给你做比喻，所以我大胆判断，可能写武侠小说的大模型更容易观察到 Aha Moment。　</section><section><img data-imgfileid="100216646" data-ratio="0.6064814814814815" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU81px1wf85iaiaGCrwM2l5dE3p4YibIvZiccINDgauEj1oSCiap7DK3Zj3mg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU81px1wf85iaiaGCrwM2l5dE3p4YibIvZiccINDgauEj1oSCiap7DK3Zj3mg/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216651" data-ratio="0.6333333333333333" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUgNghK5tZUfwSkdOTY6WiaG6tGUia8ib1yQoaqWLO4I7ibOSMYaBC59ROww/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUgNghK5tZUfwSkdOTY6WiaG6tGUia8ib1yQoaqWLO4I7ibOSMYaBC59ROww/640?wx_fmt=png&amp;from=appmsg"></section><section>我们会一同公布模型输出的采样文本文件，大家也可以在里面找到一些我们还没有发现的新奇玩意，欢迎向 Unlock-DeepSeek 团队报告你的发现。　</section><h1>展望</h1><section>在本文写作前一天，我们发现<span>另一组团队也公开了他们的 Qwen 2.5 7B 的 R1 Zero 复现结果</span><span>（https://zhuanlan.zhihu.com/p/21290410831）</span>，他们也观察到了很多有趣的结果，虽然他们的曲线非常震荡，但是也稍微能看出一点佐证我们观点的证据：似乎拉长模型输出，能带一定的计算正确率的提升。他们的工作非常棒！我们就不用去验证 7B 模型的性能了，非常环保，节能减排。大家也可以追踪观察社区其他小组的复现报告，相信开源社区的力量！　</section><section>最后嘱咐一些要点，　</section><ul><li><section><span>Math 模型不太好用，它有固有的数学输出会影响格式奖励，可能需要更长的步长才能纠正，不环保，训了一会我就停了。</span></section></li></ul><section><img data-imgfileid="100216650" data-ratio="0.5935185185185186" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWMSRz5pPTIMbwV0QEkJXooqIBBkFplcYHg9YuGGYrGzMJbSddNRRTQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWMSRz5pPTIMbwV0QEkJXooqIBBkFplcYHg9YuGGYrGzMJbSddNRRTQ/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid="100216653" data-ratio="0.6037037037037037" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeqssLiaicnGicEuHuUAbWEuDuic6wMMwtGhCicI9piaPAibyWcxAv4iafqyX3Q/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeqssLiaicnGicEuHuUAbWEuDuic6wMMwtGhCicI9piaPAibyWcxAv4iafqyX3Q/640?wx_fmt=png&amp;from=appmsg"></section><ul><li><section><span>小于 3B 的模型真不好用，没什么必要再试验了，DeepSeek 官方蒸馏的 1.5B 的推理也很烂，小模型承受了太多它不该承受的东西。我们甚至还在 0.5B 的模型看到了俄语，但是找不到图了。</span></section></li></ul><section><img data-imgfileid="100216652" data-ratio="0.600925925925926" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWibzDTonHwGgoLD9tR4aF7ZJ2IwENSzUomRdyd7UkUySvIVgEAGSgFQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWibzDTonHwGgoLD9tR4aF7ZJ2IwENSzUomRdyd7UkUySvIVgEAGSgFQ/640?wx_fmt=png&amp;from=appmsg"></section><ul><li><section><span>这种训练方式用来规范模型输出格式特别好用。</span></section></li><li><section><span>Jian Hu 报告 GRPO 有严重震荡问题</span><span>（https://zhuanlan.zhihu.com/p/14888098807）</span><span>，或许大家可以试试其他算法。</span></section></li><li><section><span>如果你的资源充足，可以试试更大的模型，希望在开源社区能够见到大家的新发现。</span></section></li><li><section><span>TRL 目前的 LoRA 模块有严重 Bug，请不要使用。</span></section></li><li><section><span>最后一点，要复现，请用 TinyZero，省钱！</span></section></li></ul><h1>完整文件获取</h1><section>Unlock-DeepSeek 团队后续会陆续发布更多关于 DeepSeek 相关工作解读的文章（马上就会发布 GRPO 解读文章），敬请关注，我们下次再见！　</section><section><section>Unlock-DeepSeek 项目主页：https://datawhalechina.github.io/unlock-deepseek/　</section><section>Github 仓库：https://github.com/datawhalechina/unlock-deepseek　</section><section>Gitee 国内仓库：https://gitee.com/anine09/unlock-deepseek　</section><section>Swanlab 实验数据：https://swanlab.cn/@anine09/datawhale-r1/overview　</section><section>模型会在晚些时候上传 HuggingFace 和 ModelScope，并在项目中公布，虽然模型本身没什么用。　</section><section>复现文件在 Datawhale-R1 文件夹。　</section><section>Unlock-DeepSeek 项目目前并不完善，并且正在快速迭代，请持续关注。　</section></section><section><img data-backh="234" data-backw="578" data-galleryid="" data-imgfileid="100216654" data-ratio="0.40555555555555556" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type="png" data-w="900" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></section><section><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”</span></span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong></section><p><mp-style-type data-value="10000"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/Z7P61IV3n4XYeC0Et_fvwg",target="_blank" rel="noopener noreferrer">原文链接</a>
