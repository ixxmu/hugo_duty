---
title: "单卡复现 DeepSeek R1 Zero教程来了！"
date: 2025-02-15T15:18:45Z
draft: ["false"]
tags: [
  "fetched",
  "Datawhale"
]
categories: ["Duty"]
---
单卡复现 DeepSeek R1 Zero教程来了！ by Datawhale
------
<div><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" data-mpa-powered-by="yiban.io"><h1 data-tool="mdnice编辑器"><section><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><section><section powered-by="xiumi.us"><section><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><section><span><strong>作者</strong><strong>：</strong><strong>邓恺俊，Datawhale成员</strong></span></section></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname="mpprofile" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname="Datawhale" data-alias="Datawhale" data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from="2" data-weuitheme="light" data-origin_num="709" data-isban="0" data-biz_account_status="0" data-index="0"></mp-common-profile></section></section></section></section></section></h1></section><section><span>项目代码可见：unlock-deepseek/Datawhale-R1（https://github.com/datawhalechina/unlock-deepseek），欢迎关注和 star！</span></section><section><span><strong><span>其余所有开源内容见文末。</span></strong></span></section><hr><section>各位同学好，我是来自 Unlock-DeepSeek 团队的邓恺俊。</section><section>之前有同学问：主播主播，你们团队的复现的 R1 Zero 确实很强，但是还是太耗算力资源，没 3 张 A800 啊，还有没有更经济更简单的方式来学习 R1 Zero 的复现呢?　</section><section>有的，兄弟，有的有的，像这样的方案还有九个（开玩笑）。今天我们来介绍一个有趣的方法，<span>能够让你在单卡复现 DeepSeek R1 Zero，甚至只用一块 4090 显卡也能轻松实现！</span>　</section><h2>为什么单卡就能复现？</h2><section>你可能会问：“原来需要 3 张 A800，如今怎么只需单卡？这其中有什么黑科技？” 答案就在于我们引入了 <span>Unsloth + LoRA</span>。</section><section>Unsloth 的核心优势在于：　</section><ul><li><section><span>强化学习算法优化</span><span>：集成了多种强化学习（RL）算法，并通过底层代码优化（如优化计算图、减少冗余操作），显著提升了大模型在推理和微调时的性能。</span></section></li><li><section><span>最新量化技术</span><span>：大幅降低显存消耗，使得原本需要多卡的大模型也能在单卡上运行。</span></section></li><li><section><span>完整的 LoRA 和 QLoRA 微调支持</span><span>：即使显存有限，也能通过少量资源复现 R1 Zero。</span></section></li></ul><section>这就为我们提供了一个成本更低、实现更简单的方案。Unsloth 官方博客提到：<span>仅需 7G VRAM，就能训练 Qwen2.5-1.5B 的模型。</span>　</section><section><span>Unsloth GitHub：https://github.com/unslothai/unsloth</span></section><h2>环境搭建</h2><h3>安装 Unsloth</h3><section>环境搭建部分在<a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700308&amp;idx=1&amp;sn=aa6324d30cc6d054c1dbb238b013b9b5&amp;scene=21#wechat_redirect" textvalue="之前的公众号文章" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">之前的公众号文章</a>中已有详细说明，这里只需在原有基础上补充安装 Unsloth 及指定版本的 trl 库即可。　</section><section><a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700308&amp;idx=1&amp;sn=aa6324d30cc6d054c1dbb238b013b9b5&amp;scene=21#wechat_redirect" textvalue="DeepSeek R1 Zero中文复现教程来了！" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">DeepSeek R1 Zero中文复现教程来了！</a><br></section><section>补充说明：在<a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700308&amp;idx=1&amp;sn=aa6324d30cc6d054c1dbb238b013b9b5&amp;scene=21#wechat_redirect" textvalue="之前公众号发布" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">之前公众号发布</a>的多卡训练代码中，错误的引入了“思考长度奖励函数”，并且没有在代码中使用 flash-attn，Unlock-DeepSeek 团队已经修复代码，请使用仓库中的最新代码，我们同时更新一版训练示意图。　</section><section><img alt="Image" data-imgfileid="100217506" data-ratio="0.5435185185185185" data-type="png" data-w="1080" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzicfWZh6T5qIeicxcfB6535UOphgTPRP1SYh0AibcCekTsicpHrNRiaal5SQ/640?wx_fmt=png&amp;from=appmsg" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzicfWZh6T5qIeicxcfB6535UOphgTPRP1SYh0AibcCekTsicpHrNRiaal5SQ/640?wx_fmt=png&amp;from=appmsg"></section><p>本文中仅展示与前文有差异的代码部分，同时我们提供了完整的训练代码，请在文末获取。　</p><blockquote data-type="2" data-url="" data-author-name="" data-content-utf8-length="37" data-source-title="" data-text="注意：由于官方的 trl 库还未更新，为了兼容 Unsloth，我们需要安装特定版本的 trl。具体命令如下：" data-editid="38xa3x3jf99cy3ljb4"><section><p>注意：为了兼容 Unsloth，我们需要安装特定版本的 trl。具体命令如下：</p></section></blockquote><p><br></p><section><section><ul><li><li><li><li><li></ul><pre data-lang="nginx"><code><span><span># 安装 unsloth 和 vllm</span></span></code><code><span><span>pip</span> install unsloth vllm</span></code><code><span><br></span></code><code><span><span># 安装指定版本的 trl（兼容 unsloth）</span></span></code><code><span>pip install trl==<span>0</span>.<span>15</span>.<span>0</span></span></code></pre></section></section><section><pre><section><span>参考自：https://docs.unsloth.ai/get-started/unsloth-notebooks</span></section></pre></section><h3>配置文件修改</h3><p>大部分配置与之前的 <span>Datawhale-R1.yaml</span> 文件保持一致。为了支持单卡复现 R1 Zero，我们做了如下调整：　</p><ul><li><section><span>LoRA 参数设置</span><span>：启用 LoRA 微调，调整 LoRA 秩数（</span><span>lora_r</span><span>）为 64（常用的选择有 8、16、32、64、128 等），并设置 </span><span>lora_alpha</span><span> 为 32。</span></section></li><li><section><span>限制回答长度</span><span>：将 </span><span>max_completion_length</span><span> 设置为 1024，以控制输出长度。</span></section></li><li><section><span>优化器调整：</span><span>优化器设置为 </span><span>adamw_8bit</span><span>，以加速训练。</span></section></li></ul><section><section>注意：为了更节省内存，这里的 <span>max_completion_length</span> 被设置为 1024，但是这可能会影响模型的发挥，如果你的资源充足，设置更高（4096、8196）可能会获得更好的效果，但是也会加重资源消耗。若内存不足可以调节 <span>vllm_gpu_memory_utilization</span>，适当降低。除此之外，如果有更多资源，可以考虑将优化器 <span>optim</span> 调整为 <span>adamw_torch</span>，这有助于更好地复现模型。　</section></section><section><section><br></section><section><ul><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li></ul><pre data-lang="bash"><code><span># LoRA 参数调整</span></code><code><span>lora_r: 64        # LoRA 秩数，选择任意大于 0 的数字！建议使用 8, 16, 32, 64, 128</span></code><code><span>lora_alpha: 32    # LoRA alpha 值</span></code><code><span><br></span></code><code><span># 训练参数</span></code><code><span>learning_rate: 1.0e-5 # 学习率，调整为1e-5</span></code><code><span><br></span></code><code><span># GRPO 算法参数</span></code><code><span>beta: 0.001       # KL 惩罚因子</span></code><code><span>optim: adamw_8bit # 使用 8bit 优化器以加速训练</span></code><code><span>max_prompt_length: 256       # 输入 prompt 的最大长度</span></code><code><span>max_completion_length: 1024  # 输出回答长度，包含推理思维链</span></code><code><span>num_generations: 4</span></code><code><span>use_vllm: true               # 启用 vLLM 加速推理</span></code><code><span>vllm_gpu_memory_utilization: 0.4  # vLLM 的 GPU 内存利用率（内存紧张时可适当降低）</span></code></pre></section><p><span>LoRA微调参考：https://zhuanlan.zhihu.com/p/663557294</span></p></section><h2>启动训练</h2><p>启动训练的代码很简单，由于我们只需要单卡，不需要涉及到配置复杂的 Accelerate 库，直接运行以下代码即可运行。　</p><section><ul><li></ul><pre data-lang="css"><code><span><span>python</span> <span>train_Datawhale-R1_unsloth</span><span>.py</span> <span>--config</span> <span>Datawhale-R1_unsloth</span><span>.yaml</span></span></code></pre></section><h2>训练代码优化解读</h2><p>基于 Unsloth 框架，我们对原始代码做了简化和优化。主要思路有两点：　</p><h3>打补丁提升训练速度</h3><p>在执行强化学习训练的代码之前，我们添加了两行代码，利用 <span>PatchFastRL</span> 函数对某些 RL 算法（如 GRPO）进行“打补丁”。这个操作实际上在底层优化了计算图、减少了冗余计算，从而加速训练过程。　</p><section><ul><li><li></ul><pre data-lang="python"><code><span>from unsloth import FastLanguageModel, PatchFastRL</span></code><code><span>PatchFastRL("GRPO", FastLanguageModel)  # 对 GRPO 算法打补丁</span></code></pre></section><h3>GRPO 训练函数的改进</h3><section>除此之外，我们还改进了 <span>grpo_function</span> 里面的函数，在这之中进行了一些优化，具体在代码的 14～34 行中，具体来说，我们加入以下两个方式：</section><ul><li><section><span>模型加载</span><span>：通过 </span><span>FastLanguageModel.from_pretrained</span><span> 方法加载预训练模型，并启用 vLLM 快速推理，同时支持 4 位加载（或 LoRA 16 位）。</span></section></li><li><section><span>PEFT 微调</span><span>：利用 </span><span>get_peft_model</span><span> 方法对模型应用 LoRA 微调，指定了目标模块、LoRA 参数以及梯度检查点，确保在有限显存条件下依然能有效训练。</span></section></li></ul><section><ul><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li></ul><pre data-lang="python"><code><span><span># 定义 GRPO 训练函数</span></span></code><code><span><span><span>def</span> <span>grpo_function</span><span>(</span></span></span></code><code><span>    model_args: ModelConfig,</span></code><code><span>    dataset_args: DatasetArguments,</span></code><code><span>    training_args: GRPOConfig,</span></code><code><span>    callbacks: List,</span></code><code><span><span>)</span>:</span></code><code><span>    <span># 记录模型参数</span></span></code><code><span>    logger.info(<span>f"Model parameters <span>{model_args}</span>"</span>)</span></code><code><span>    <span># 记录训练/评估参数</span></span></code><code><span>    logger.info(<span>f"Training/evaluation parameters <span>{training_args}</span>"</span>)</span></code><code><span><br></span></code><code><span>    <span># 从预训练模型加载模型和分词器</span></span></code><code><span>    model, tokenizer = FastLanguageModel.from_pretrained(</span></code><code><span>        model_name=model_args.model_name_or_path,  <span># 模型名称或路径</span></span></code><code><span>        fast_inference=<span>True</span>,  <span># 启用 vLLM 快速推理</span></span></code><code><span>        load_in_4bit=<span>True</span>,  <span># 是否以 4 位加载模型，False 表示使用 LoRA 16 位</span></span></code><code><span>        max_lora_rank=model_args.lora_r,  <span># 设置 LoRA 的最大秩</span></span></code><code><span>        max_seq_length=training_args.max_completion_length,  <span># 设置最大序列长度</span></span></code><code><span>        gpu_memory_utilization=training_args.vllm_gpu_memory_utilization,  <span># GPU 内存利用率，若内存不足可减少</span></span></code><code><span>        attn_implementation=model_args.attn_implementation, <span># 设置注意力实现方式 flash attention</span></span></code><code><span>    )</span></code><code><span><br></span></code><code><span>    <span># PEFT 模型</span></span></code><code><span>    model = FastLanguageModel.get_peft_model(</span></code><code><span>        model,</span></code><code><span>        r = model_args.lora_r, </span></code><code><span>        target_modules = [</span></code><code><span>            <span>"q_proj"</span>, <span>"k_proj"</span>, <span>"v_proj"</span>, <span>"o_proj"</span>, <span># 如果 OOM 内存不足，可以移除 QKVO</span></span></code><code><span>            <span>"gate_proj"</span>, <span>"up_proj"</span>, <span>"down_proj"</span>,</span></code><code><span>        ],  </span></code><code><span>        lora_alpha = model_args.lora_alpha,  <span># 设置 LoRA 的 alpha 值</span></span></code><code><span>        use_gradient_checkpointing = <span>"unsloth"</span>,  <span># 启用 unsloth 的梯度检查</span></span></code><code><span>        random_state = training_args.seed,  <span># 设置随机种子</span></span></code><code><span>    )</span></code></pre></section><section>如果遇到 Out of Memory 显存不足问题，可以移除 <span>target_modules</span> 中的 <span>"q_proj", "k_proj", "v_proj", "o_proj"</span>。<span><br></span></section><p><span>参考自：https://unsloth.ai/blog/r1-reasoning</span></p><p><span>模型量化参考：LLM量化综合指南（8bits/4bits）https://zhuanlan.zhihu.com/p/671007819</span></p><h2>训练结果与一些思考</h2><section>以下是训练结果的部分截图，大致与我们复现 Tiny Zero 和 Mini R1 的结果类似，这里就不再做详细分析。　</section><section><img alt="Image" data-imgfileid="100217505" data-ratio="0.1814814814814815" data-type="png" data-w="1080" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzzHrpkBGAOTfeRMyYUwqyHFTQQiaWgyROROgJMy3icKO6zgO8v0QzrbOA/640?wx_fmt=png&amp;from=appmsg" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzzHrpkBGAOTfeRMyYUwqyHFTQQiaWgyROROgJMy3icKO6zgO8v0QzrbOA/640?wx_fmt=png&amp;from=appmsg"></section><section>接下来分享一些学习 R1 Zero 过程中的思考（非严谨学术研究，个人观点，仅供参考）。　</section><h3>Aha moment 是 RL 训练的结果吗？</h3><section>在开始研究之前，我对 Aha moment（顿悟时刻）这个概念充满好奇，这仿佛是 DeepSeek 在经过 RL 训练后突然获得的超能力。但深入学习阅读了 oat 的文章后，发现 Aha moment 并不是凭空出现的，它可能在 base 模型和 SFT 阶段就已经埋下了种子。RL 训练做的事情，更像是一个"放大器"，通过设计的奖励机制，最大化了模型产生顿悟时刻的概率。换句话说，RL 训练将模型原本浅层的自我反思能力，转化为更有深度和效果的思考过程。　</section><section><span>参考OAT文章：There May Not be Aha Moment in R1-Zero-like Training — A Pilot Study：https://oatllm.notion.site/oat-zero　</span></section><h3>思考长度越长越有效吗？</h3><section>在社区中有一种普遍的看法：RL 训练让模型的输出变得更长，从而提升效果。这个观点确实有一定道理，因为 RL 强化了模型的思考过程，生成更多的 token 是自然的结果。</section><section>然而，问题是：<span>更长的思考真的意味着更好的结果吗？</span>　</section><section>在复现 Tiny Zero 的过程中，我观察到一个有趣的现象：<span>token 数量呈现先降后升的趋势</span>。这个现象可以这样解释：最初，由于存在格式奖励（format reward），模型必须保证格式正确，然而长度过长的输出比较难学习到答案的格式，并且会包含很多对解决任务无用的 token，所以 token 数量自然先会下降，模型先学简单的格式，保留有利于正确计算的 token，再去学复杂的计算，先简后繁；随着训练的进行，模型开始进行更多的尝试和反思以得出正确答案，输出长度逐渐增加并趋于稳定。这一观察也印证了 OAT 的结论：<span>输出长度与自我反思的质量并不一定存在线性关联。</span></section><h3>S1 文章的一些结论和思考</h3><section>最近我也看了李飞飞团队的 S1 文章，详细分析了其方法和 R1 Zero 的不同。总的来说，<span>S1</span> 通过少量高质量数据（约 1k + SFT + 设计 Prompt）进行训练，而 <span>R1 Zero</span> 则是通过基础训练（Base）加 RL 强化训练完成的。在 S1 中，他们采用了 <span>budget forcing</span> 方法，在测试时强制设定最大和最小的思考 token 数量。具体而言：　</section><ul><li><section><span>通过添加 </span><span>"end-of-thinking token 分隔符"</span><span> 和 </span><span>"Final Answer"</span><span> 来控制思考上限；</span></section></li><li><section><span>通过禁止生成分隔符并添加 </span><span>"wait"</span><span> 提示词来控制思考下限。</span></section></li></ul><section>实验结果表明，适度增加思考 token 数量确实能够提升模型在 AIME24 基准测试上的表现。然而，他们也发现，过度抑制思考结束反而会导致模型陷入无效循环。这个发现非常符合直觉：就像人类的思考一样，<span>简单问题（比如数一个单词中的字母数量）并不需要过度思考</span>，而真正需要延长思考时间的，往往是那些较为复杂的问题。　</section><section><span>s1参考阅读：<a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2652562988&amp;idx=2&amp;sn=85722bbd6e6142e1e24c3af68006b08e&amp;scene=21#wechat_redirect" textvalue="16张H100训26分钟，超越o1-preview！李飞飞等用1K样本，揭秘测试时Scaling" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">16张H100训26分钟，超越o1-preview！李飞飞等用1K样本，揭秘测试时Scaling</a></span></section><h2>总结与展望</h2><section>首先再次感谢 Unsloth 的优化和社区小伙伴的努力，这不仅使得大模型的训练和推理更加高效，还大幅降低了显存消耗，使得即便是仅一块显卡，也能轻松完成 R1 Zero 的复现，这也提供了一个更经济、更简便的复现方案，也为低资源环境下的大模型应用开辟了新的可能。　</section><section>同时我们也计划深入探讨 R1 的蒸馏方法，以进一步降低模型的计算需求并提高其可扩展性。此外，我们也将持续优化代码和算法，推动更多开源社区的创新和合作。我们欢迎大家参与讨论和分享，期待和大家一起在开源社区中共创更多精彩内容。　</section><h1>完整文件获取<span></span></h1><section>Unlock-DeepSeek 团队后续会陆续发布更多关于 DeepSeek 相关工作解读的文章，敬请关注，我们下次再见！　</section><section><section>Unlock-DeepSeek 项目主页：https://datawhalechina.github.io/unlock-deepseek/　</section><section>Github 仓库：https://github.com/datawhalechina/unlock-deepseek　</section><section>Gitee 国内仓库：https://gitee.com/anine09/unlock-deepseek　</section><section>Swanlab 实验数据：https://swanlab.cn/@Kedreamix/Datawhale-R1-by_Kedreamix/runs/sqxeo1i3v8hgzclm3nwkk</section><section>复现文件在 Datawhale-R1 文件夹，请仔细阅读 Datawhale-R1/README.md。</section><section>Unlock-DeepSeek 项目目前并不完善，并且正在快速迭代，请持续关注。　</section></section><section><img alt="图片" data-backh="234" data-backw="578" data-galleryid="" data-imgfileid="100217508" data-ratio="0.40555555555555556" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type="png" data-w="900" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”</span></span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong></section><p><mp-style-type data-value="10000"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/0Q369GFqA_VguWeiL0MwDg",target="_blank" rel="noopener noreferrer">原文链接</a>
