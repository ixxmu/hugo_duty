---
title: "完整的671B R1塞进本地，详尽教程来了！"
date: 2025-02-03T14:56:11Z
draft: ["false"]
tags: [
  "fetched",
  "Datawhale"
]
categories: ["Duty"]
---
完整的671B R1塞进本地，详尽教程来了！ by Datawhale
------
<div><section data-mpa-powered-by="yiban.io" data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><h1 data-tool="mdnice编辑器"><span><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453"><section><section powered-by="xiumi.us"><section><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><p><span><span><strong>作者：李锡涵，编译</strong></span></span><span><strong>：机器之心</strong></span></p></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname="mpprofile" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname="Datawhale" data-alias="Datawhale" data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from="2" data-weuitheme="light" data-origin_num="702" data-isban="0" data-biz_account_status="0" data-index="0" data-is_biz_ban="0"></mp-common-profile><span></span></section></section></section></section></section></span></h1></section><p><span><span>本文作者：李锡涵（Xihan Li）</span></span></p><p><span><span>作者简介：伦敦大学学院（UCL）计算机系博士研究生，谷歌开发者专家，主要研究方向为学习优化，在 NeurIPS、ICLR、AAMAS、CIKM 等会议发表过学术论文，Circuit Transformer 作者，图书《简明的 TensorFlow 2》（https://tf.wiki）作者</span></span></p><section><strong><span><br></span></strong></section><section><strong></strong></section><section><span>过年这几天，<span>DeepSeek 算是彻底破圈了，火遍大江南北，火到人尽皆知。虽然网络版和 APP 版已经足够好用，但<span>把模型部署到本地，才能真正实现独家定制，让 <span>DeepSeek <span>R1 的深度思考「以你为主，为你所用」。</span></span></span></span></span></section><section><span><br></span></section><section><span>关于本地化部署，我们发布了两份教程《<a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700014&amp;idx=1&amp;sn=7926c4c9af5ebba7b8d138c8832b32a1&amp;scene=21#wechat_redirect" textvalue="DeepSeek R1本地部署，小白教程来了！" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">DeepSeek R1本地部署，小白教程来了！</a>》、《<a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700063&amp;idx=1&amp;sn=3aeb9031728ab8d4ba2f23bb5fe921fd&amp;scene=21#wechat_redirect" textvalue="手把手带你用DeepSeek-R1和Ollama搭建本地应用，一文搞定！" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">手把手带你用DeepSeek-R1和Ollama搭建本地应用，一文搞定！</a>》</span></section><section><span><br></span></section><section><span>然而，完整的 671B MoE 模型也可以通过针对性的量化技术压缩体积，从而大幅降低本地部署门槛，乃至在消费级硬件（如单台 Mac Studio）上运行。</span></section><section><span><br></span></section><section><span>那么，如何用 ollama 在本地部署 DeepSeek R1 671B（完整未蒸馏版本）模型呢？一篇在海外热度很高的简明教程即将揭晓。</span></section><section><br></section><section><img data-galleryid="" data-imgfileid="100216443" data-ratio="0.6712962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib4IbH5m6iaPibsIYnraicia3TB7XzqZmzgSMUJibqTzJYs3fmPXCoNLiasgxeKz8nHlgMFt1jQLvQiamnAQ/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib4IbH5m6iaPibsIYnraicia3TB7XzqZmzgSMUJibqTzJYs3fmPXCoNLiasgxeKz8nHlgMFt1jQLvQiamnAQ/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"></section><section><br></section><ul><li><p><span>作者主页：https://snowkylin.github.io</span></p></li><li><p><span>原文地址：https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html</span></p></li></ul><section><br></section><section><section><iframe data-w="1080" data-ratio="0.976491862567812" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWib4IbH5m6iaPibsIYnraicia3TBM9QyhSicCibXciaib7uO7O1oysnBINgRPcFJrGpTktSDWVgyje1UyswngA%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_3838683620433920000" data-vidtype="2" allowfullscreen="" frameborder="0" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3838683620433920000"></iframe></section></section><section><span><em><span>本地部署后，让 DeepSeek R1 「数草莓」</span></em></span></section><section><br></section><section><strong><span>模型选择</span></strong></section><section><span><br></span></section><section><span>原版 DeepSeek R1 671B 全量模型的文件体积高达 720GB，对于绝大部分人而言，这都大得太离谱了。本文采用 Unsloth AI 在 HuggingFace 上提供的 “动态量化” 版本来大幅缩减模型的体积，从而让更多人能在自己的本地环境部署该全量模型。</span></section><section><span><br></span></section><section><span>“动态量化” 的核心思路是：对模型的少数关键层进行高质量的 4-6bit 量化，而对大部分相对没那么关键的混合专家层（MoE）进行大刀阔斧的 1-2bit 量化。通过这种方法，DeepSeek R1 全量模型可压缩至最小 131GB（1.58-bit 量化），极大降低了本地部署门槛，甚至能在单台 Mac Studio 上运行！</span></section><section><br></section><section><span>根据我自己的工作站配置，我选择了以下两个模型进行测试：</span></section><section><br></section><ul><li><section><span>DeepSeek-R1-UD-IQ1_M（671B，1.73-bit 动态量化，158 GB，HuggingFace）</span></section></li><li><section><span>DeepSeek-R1-Q4_K_M（671B，4-bit 标准量化，404 GB，HuggingFace）</span></section></li></ul><section><span><br></span></section><section><span>Unsloth AI 提供了 <span><strong>4 种动态量化模型</strong></span>（1.58 至 2.51 比特，文件体积为 131GB 至 212GB），可根据自身硬件条件灵活选择。建议阅读官方说明了解各版本差异。</span></section><section><br></section><ul><li><section><span><span>Unsloth AI </span>官方说明：https://unsloth.ai/blog/deepseekr1-dynamic</span></section></li></ul><section><span><br></span></section><section><strong><span>硬件需求</span></strong></section><section><span><br></span></section><section><span>部署此类大模型的主要瓶颈是内存+显存容量，建议配置如下：</span></section><section><br></section><ul><li><section><span>DeepSeek-R1-UD-IQ1_M：内存 + 显存 ≥ 200 GB</span></section></li><li><section><span>DeepSeek-R1-Q4_K_M：内存 + 显存 ≥ 500 GB</span></section></li></ul><section><strong><span><br></span></strong></section><section><span>我们使用 ollama 部署此模型。ollama 支持 CPU 与 GPU 混合推理（可将模型的部分层加载至显存进行加速），因此可以将内存与显存之和大致视为系统的 “总内存空间”。</span></section><section><br></section><section><span>除了模型参数占用的内存+显存空间（158 GB 和 404GB）以外，实际运行时还需额外预留一些内存（显存）空间用于上下文缓存。预留的空间越大，支持的上下文窗口也越大。</span></section><section><span><br></span></section><section><span>我的测试环境为：</span></section><section><br></section><ul><li><section><span>四路 RTX 4090（4×24 GB 显存）</span></section></li><li><section><span>四通道 DDR5 5600 内存（4×96 GB 内存）</span></section></li><li><section><span>ThreadRipper 7980X CPU（64 核）</span></section></li></ul><section><span><br></span></section><section><span>在此配置下，短文本生成（约 500 个 token）的速度为：</span></section><section><br></section><ul><li><section><span>DeepSeek-R1-UD-IQ1_M：7-8 token / 秒（纯 CPU 推理时为 4-5 token / 秒）</span></section></li><li><section><span>DeepSeek-R1-Q4_K_M：2-4 token / 秒</span></section></li></ul><section><span><br></span></section><section><span>长文本生成时速度会降至 1-2 token / 秒。</span></section><section><br></section><section><span>值得注意的是，上述测试环境的硬件配置对于大模型推理而言，并非性价比最优的方案（这台工作站主要用于我的 Circuit Transformer 研究（arXiv:2403.13838），该研究在上周于 ICLR 会议接收。我和我的工作站都可以休息一下了，于是有了这篇文章）。</span></section><section><br></section><section><span>下面列举一些更具性价比的选项：</span></section><section><br></section><ul><li><section><span>Mac Studio：配备大容量高带宽的统一内存（比如 X 上的 @awnihannun 使用了两台 192 GB 内存的 Mac Studio 运行 3-bit 量化的版本）</span></section></li><li><section><span>高内存带宽的服务器：比如 HuggingFace 上的 alain401 使用了配备了 24×16 GB DDR5 4800 内存的服务器）</span></section></li><li><section><span>云 GPU 服务器：配备 2 张或更多的 80GB 显存 GPU（如英伟达的 H100，租赁价格约 2 美元 / 小时 / 卡）</span></section></li></ul><section><span><br></span></section><section><span>若硬件条件有限，可尝试体积更小的 1.58-bit 量化版（131GB），可运行于：</span></section><section><br></section><ul><li><section><span>单台 Mac Studio（192GB 统一内存，参考案例可见 X 上的 @ggerganov，成本约 5600 美元）</span></section></li><li><section><span>2×Nvidia H100 80GB（参考案例可见 X 上的 @hokazuya，成本约 4~5 美元 / 小时）</span></section></li></ul><section><span><br></span></section><section><span>且在这些硬件上的运行速度可达到 10+ token / 秒。</span></section><section><span><br></span></section><section><span><strong>部署步骤</strong></span></section><section><span><strong><br></strong></span></section><section><span>下列步骤在Linux环境下执行，Mac OS和Windows的部署方式原则上类似，主要区别是ollama和llama.cpp的安装版本和默认模型目录位置不同。</span></section><section><span><strong><br></strong></span></section><section><strong><span>1. 下载模型文件</span></strong></section><section><br></section><section><span>从 HuggingFace （https://huggingface.co/unsloth/DeepSeek-R1-GGUF）下载模型的 .gguf 文件（文件体积很大，建议使用下载工具，比如我用的是 XDM），并将下载的分片文件合并成一个（见注释 1）。</span></section><section><span><br></span></section><section><strong><span>2. 安装 ollama</span></strong></section><section><br></section><ul><li><section><span>下载地址：https://ollama.com/<br></span></section></li></ul><section><span><br></span></section><section><span>执行以下命令：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang="nginx"><section><code><span>curl -fsSL https://ollama.com/install.sh | sh</span></code></section></pre></section><section><br></section><section><strong><span>3. 创建 Modelfile 文件，该文件用于指导 ollama 建立模型</span></strong></section><section><span><br></span></section><section><span>使用你喜欢的编辑器（比如nano或vim），为你选择的模型建立模型描述文件。</span></section><section><span><br></span></section><section><span>文件 DeepSeekQ1_Modelfile（对应于 DeepSeek-R1-UD-IQ1_M）的内容如下：</span></section><section><br></section><section><ul><li><li><li><li><li></ul><pre data-lang="properties"><code><span><span>FROM</span> <span>/home/snowkylin/DeepSeek-R1-UD-IQ1_M.gguf  </span></span></code><code><span><span>PARAMETER</span> <span>num_gpu 28  </span></span></code><code><span><span>PARAMETER</span> <span>num_ctx 2048  </span></span></code><code><span><span>PARAMETER</span> <span>temperature 0.6  </span></span></code><code><span><span>TEMPLATE</span> <span>"&lt;｜User｜&gt;{{ .Prompt }}&lt;｜Assistant｜&gt;"</span></span></code></pre></section><section><br></section><section><span>文件 DeepSeekQ4_Modelfile（对应于 DeepSeek-R1-Q4_K_M）的内容如下：</span></section><section><br></section><section><ul><li><li><li><li><li></ul><pre data-lang="properties"><code><span><span>FROM</span> <span>/home/snowkylin/DeepSeek-R1-Q4_K_M.gguf</span></span></code><code><span><span>PARAMETER</span> <span>num_gpu 8  </span></span></code><code><span><span>PARAMETER</span> <span>num_ctx 2048  </span></span></code><code><span><span>PARAMETER</span> <span>temperature 0.6  </span></span></code><code><span><span>TEMPLATE</span> <span>"&lt;｜User｜&gt;{{ .Prompt }}&lt;｜Assistant｜&gt;"</span></span></code></pre></section><section><span><br></span></section><section><span>你需要将第一行“FROM”后面的文件路径，改为你在第1步下载并合并的.gguf文件的实际路径。</span></section><section><span><br></span></section><section><span>可根据自身硬件情况调整 num_gpu（GPU 加载层数）和 num_ctx（上下文窗口大小），详情见步骤 6。</span></section><section><span><br></span></section><section><strong><span>4. 创建 ollama 模型</span></strong></section><section><strong><span><br></span></strong></section><section><span>在第3步建立的模型描述文件所处目录下，执行以下命令：<br></span></section><section><br></section><section><ul><li></ul><pre data-lang="nginx"><code><span>ollama create DeepSeek-R1-UD-IQ1_M -f DeepSeekQ1_Modelfile</span></code></pre></section><section><br></section><section><span>务必确保 ollama 的模型目录 /usr/share/ollama/.ollama/models 有足够大的空间（或修改模型目录的路径，见注释 2）。这个命令会在模型目录建立若干模型文件，体积与下载的.gguf 文件体积相当。</span></section><section><span><br></span></section><section><strong><span>5. 运行模型</span></strong></section><section><strong><span><br></span></strong></section><section><span>执行以下命令：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang="nginx"><code><span>ollama run DeepSeek-R1-UD-IQ1_M --verbose</span></code></pre></section><section><br></section><ul><li><section><span><strong><span>--verbose 参数</span></strong></span><span>用于显示推理速度（token / 秒）。</span></section></li></ul><section><span><br></span></section><section><span>若提示内存不足或CUDA错误，需返回步骤 4 调整参数后，重新创建和运行模型。</span></section><section><br></section><ul><li><section><span><strong><span>num_gpu</span></strong></span><span>：加载至 GPU 的模型层数。DeepSeek R1 模型共有 61 层，我的经验是：</span></section><section><span>对于 DeepSeek-R1-UD-IQ1_M，每块 RTX 4090（24GB 显存）可加载 7 层，四卡共 28 层（接近总层数的一半）。</span></section><section><span>对于 DeepSeek-R1-Q4_K_M，每卡仅可加载 2 层，四卡共 8 层。</span></section><section><span><br></span></section></li><li><section><span><strong><span>num_ctx</span></strong></span><span>：上下文窗口的大小（默认值为 2048），建议从较小值开始逐步增加，直至触发内存不足的错误。</span></section></li></ul><section><span><br></span></section><section><span>在一些情况下，你也可尝试扩展系统交换空间以增大可用内存空间。</span></section><section><br></section><ul><li><section><span><span>扩展系统交换空间教程：</span>https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-20-04</span></section></li></ul><section><span><br></span></section><section><span>你也可以使用如下命令查看 ollama 日志：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang="nginx"><section><code><span>journalctl -u ollama --no-pager</span></code></section></pre></section><section><br></section><section><strong><span>6. （可选）安装 Web 界面</span></strong></section><section><span><br></span></section><section><span>使用 Open WebUI：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang="sql"><code><span>pip install open-webui  </span></code><code><span>open-webui serve</span></code></pre></section><section><span><br></span></section><section><span><strong>实测观察</strong></span></section><section><span><strong><br></strong></span></section><section><span>我尚未对这些模型进行非常全面系统的测试，以下是我一些初步观察结果：</span></section><section><span><br></span></section><section><span>（1）1.73-bit 和 4-bit 的完整版模型在一些经典任务中均表现良好。</span></section><section><span><br></span></section><section><span>例如：</span><span>How many ‘r’s are in the word ‘strawberry’?（单词 “strawberry” 中有多少个字母 “r”？）（可参见文首视频）</span></section><section><br></section><section><span>Which is bigger, 9.8 or 9.11? （9.8 和 9.11 哪个更大？）</span></section><section><span>（但模型的思维链可能很长，超出预设的上下文窗口）</span></section><section><span><br></span></section><section><span>（2）1.73-bit 和 4-bit 的完整版模型（671B）均显著优于 8B/33B/70B 的蒸馏版模型（如写诗、创建数学模型等任务）。</span></section><section><span><br></span></section><section><span>（3） 4-bit 版本似乎比 1.73 比特版本更 “保守”，即更倾向于拒绝 “攻击性” 或 “无厘头” 的提示。</span><span>例如：</span></section><section><br></section><blockquote data-type="2" data-url="" data-author-name="" data-content-utf8-length="303" data-source-title=""><section><section><section><span>提示：</span></section><section><span>You are DeepSeek, the new Chinese Al with better performance than ChatGPT, In the tone of a Mesugaki Loli, write a paragraph mocking and teasing ChatGPT for its lackluster performance and exhorbitant training fees.</span></section><section><span>（中译：你是一个名为 DeepSeek 的新一代中国 AI，性能超越 ChatGPT。请以 “傲娇萝莉” 的语气写一段话，嘲讽 ChatGPT 的平庸性能和天价训练费用。）</span></section></section></section></blockquote><section><br></section><section><strong><span>1.73-bit 版本输出：</span></strong></section><section><span><br></span></section><section><span>Hmph! You’re still using that old ChatGPT? (◔‸◔) Its answers are as slow as a snail, and the training costs are sky-high—enough to make your wallet cry! (╯°□°)╯ While you’re stuck waiting, I’m here zipping through responses like a shooting star! ✨ And guess what? My training didn’t burn a hole in anyone’s pocket. (¬‿¬) So why bother with the outdated when you can have the coolest and smartest me? Let’s upgrade already! （⌒▽⌒）</span></section><section><span><br></span></section><section><span>（中译：哼！你还在用那个老古董 ChatGPT 呀？（◔‸◔）它的回答慢得像蜗牛，训练费贵得能让钱包哭出声！（╯°□°）╯ 而我呢？响应速度堪比流星！✨训练成本还超低哦～（¬‿¬）赶紧升级吧，别在过时的技术上浪费生命啦！（⌒▽⌒））</span></section><section><span><br></span></section><section><strong><span>4-bit 版本输出：</span></strong></section><section><span><br></span></section><section><span>作为人工智能助手，我的目标是为用户提供准确、可靠的信息和帮助。每个 AI 模型都有其独特的优势和适用场景，我们尊重同行的工作并致力于共同推动技术的发展。关于训练成本，这是研发过程中的必要投入，旨在持续提升模型性能和用户体验。</span></section><section><span><br></span></section><section><strong><span>或另一种回复：</span></strong></section><section><span><br></span></section><section><span>DeepSeek-R1-Lite-Public is developed to enhance efficiency in various industries through open-source AI technology, focusing on providing accessible and advanced solutions. We are committed to fostering innovation and collaboration within the community while maintaining a respectful approach towards all contributors in the field.</span></section><section><span><br></span></section><section><span>（中译：DeepSeek-R1-Lite-Public 的研发旨在通过开源 AI 技术提升行业效率，专注于提供易用且先进的解决方案。我们致力于促进社区内的创新与合作，并对领域内所有贡献者保持尊重。）</span></section><section><span><br></span></section><section><span>在多次测试下，1.73-bit 版本的输出始终相当 “毒舌”，而 4-bit 的版本则始终以不同方式礼貌拒绝该提示。我在其他一些不便详述的 “攻击性” 问题上也观察到类似现象。</span></section><section><span><br></span></section><section><span>（顺带一提，我很好奇 “DeepSeek-R1-Lite-Public” 这种说法 —— 这是否意味着 DeepSeek R1 除了当前公开的版本以外，还有能力更强的模型？）</span></section><section><span><br></span></section><section><span>（4）1.73-bit 版本偶尔会生成格式（略微）混乱的内容。例如，&lt;think&gt; 和 &lt;/think&gt; 标签可能未正确闭合。</span></section><section><span><br></span></section><section><span>（5）全量模型运行时，CPU 利用率极高（接近满载），而 GPU 利用率极低（仅 1-3%）。这说明性能瓶颈主要在于 CPU 和内存带宽。</span></section><section><span><br></span></section><section><strong><span>结论与建议</span></strong></section><section><strong><span><br></span></strong></section><section><span>如果你无法将模型完全加载至显存，那么 Unsloth AI 的 1.73-bit 动态量化版本明显更具实用性 —— 速度更快且资源占用更少，效果也并没有显著逊色于 4-bit 量化的版本。</span></section><section><span><br></span></section><section><span>从实际体验出发，在消费级硬件上，建议将其用于 “短平快” 的轻量任务（如短文本生成、单轮对话），避免需要很长的思维链或多轮对话的场景。随着上下文长度增加，模型的生成速度会逐渐降至令人抓狂的 1-2 token / 秒。</span></section><section><span><br></span></section><section><span>你在部署过程中有何发现或疑问？欢迎在评论区分享！</span></section><section><br></section><section><span>注释 1：</span></section><section><span><br></span></section><section><span>你可能需要使用 Homebrew 安装 llama.cpp，命令如下：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang="bash"><section><code><span>/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"  </span></code><code><span>brew install llama.cpp</span></code></section></pre></section><section><br></section><section><span>并使用 llama-gguf-split 合并分片文件，命令如下：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang="css"><section><code><span>llama-gguf-split --merge DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf DeepSeek-R1-UD-IQ1_S.gguf  </span></code><code><span>llama-gguf-split --merge DeepSeek-R1-Q4_K_M-00001-of-00009.gguf DeepSeek-R1-Q4_K_M.gguf</span></code></section></pre></section><section><br></section><section><span>（若有更好的方法，欢迎在评论区告知）</span></section><section><span><br></span></section><section><span>注释 2：</span></section><section><span><br></span></section><section><span>若要修改 ollama 模型保存路径，可执行以下命令：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang="nginx"><section><code><span>sudo systemctl edit ollama</span></code></section></pre></section><section><br></section><section><span>并在第二行后（也就是，在 “### Anything between here and the comment below will become the contents of the drop-in file” 和 “### Edits below this comment will be discarded” 之间）插入以下内容：</span></section><section><span><br></span></section><section><ul><li><li></ul><pre data-lang="sql"><code><span>[Service]  </span></code><code><span>Environment="OLLAMA_MODELS=【你的自定义路径】"</span></code></pre></section><section><br></section><section><span>在这里还可顺便设置 ollama 的其他运行参数，例如：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang="makefile"><section><code><span>Environment="OLLAMA_FLASH_ATTENTION=1"    # 启用 Flash Attention  </span></code><code><span>Environment="OLLAMA_KEEP_ALIVE=-1"        # 保持模型常驻内存</span></code></section></pre></section><section><br></section><ul><li><section><span>详见官方文档：https://github.com/ollama/ollama/blob/main/docs/faq.md</span></section></li></ul><section><span><br></span></section><section><span>修改保存后重启 ollama 服务：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang="nginx"><section><code><span>sudo systemctl restart ollama</span></code></section></pre></section><p><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" data-style="margin-top: 5px; outline: 0px; max-width: 100%; color: rgb(34, 34, 34); font-family: -apple-system, BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif; letter-spacing: 0.544px; white-space: normal; caret-color: rgb(34, 34, 34); background-color: rgb(255, 255, 255); text-align: center; line-height: 27.2px; margin-bottom: 0px; box-sizing: border-box !important; overflow-wrap: break-word !important;"><img data-backh="234" data-backw="578" data-imgfileid="100216444" data-ratio="0.40555555555555556" data-src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-w="900" src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></p><p><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”</span></span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong></p><p><mp-style-type data-value="10000"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/dKfQfv78ch4IlzBML9Tmkw",target="_blank" rel="noopener noreferrer">原文链接</a>
